---
title: "Generalised Linear Models"
format: html
---



The log likehood function for the canonical exponential family, in terms of $X$ and $\beta$, can be written generally as:
$$
l_n(\beta; Y, X) \propto \Sigma_i \frac{Y_i h(X_i^\top \beta) - b(X_i^\top \beta)}{\phi}
$$

In the case of a canonical link function $h$ is the identity function.  
$b$ is the cumulant generating function. Each sub-family of distributions belonging to the exponential family has a unique $b$.  

If $g$ is the link function, then $h = (g \circ b')^{-1} = b'^{-1}(g^{-1}(X_i^\top \beta))$.  

In this case of the canonical link, we define the link such that $g = (b')^{-1}$, which indicates that $g^{-1}$ and $(b')^{-1}$ cancel out.  

$\phi$ is the dispersion paramater, and its value varies across the exponential families.  

# Example 1 - Bernoulli (logistic regression) on the kyphosis dataset

Model: $Y|X \sim Bern(\mu(X))$

We will use the canonical logit link, so $h$ is identity.

The cumulant generating function is $b = \log(1 + e^\theta)$.  

The logit link function is $g = \log(\frac{\theta}{1 - \theta})$  

$\phi$ (the dispersion paramater) is $1$ for the Bernoulli family.

Thus, we have the simple log-likelihood:  
$$
l_n(\beta; Y, X) = \Sigma_i (~ Y_i ~ X_i^\top \beta - \log(1 + e^{X_i^\top \beta}) ~)
$$

Maximizing this function with respect to $\beta$ will give us the parameters that make this model most likely given the data ($X$ and $Y$)


**DATA**:  

The kyphosis dataset represents data on children who have had corrective spinal surgery.  

Variables:

- kyphosis: indicates if kyphosis, a type of deformation, is present after the operation  
- age: age of patient in months  
- number: the number of vertebrae involved  
- start: the number of the first (topmost) verterba operated on  


```{r}
kyphosis <- read.csv("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/rpart/kyphosis.csv")
names(kyphosis) <- tolower(names(kyphosis))
kyphosis$present <- kyphosis$kyphosis == "present"
kyphosis$rownames <- NULL
xvars <- c("age", "number", "start")

head(kyphosis)


# GLM (logistic regression)
# Y|X ~ Bernoulli
# canonical logit link
ll.bern <- function(x, y) {
  nx <- ncol(x)
  x <- as.matrix(x, ncol = if (is.null(nx)) 1 else nx)
  x <- cbind(1, x) # intercept
  y <- as.matrix(y, ncol = 1)
  function(b) {
    b <- as.matrix(b, ncol = 1)
    xb <- x %*% b
    sum(y * xb - log(1 + exp(xb)))
  }
}

ll <- ll.bern(x = kyphosis[, xvars],
              y = kyphosis$present)

o <- optim(par = rep(0.5, length(xvars) + 1), # initial param values
           fn = ll, # fn to optimize
           control = list(fnscale = -1)) # optim is a minimizer, we need max

# final param values
setNames(o$par, c("Intercept", xvars))

# compute on "new" data...
testage <- data.frame(intercept = 1,
                      age = 1:600, # reasonable values + some
                      number = mean(kyphosis$number), # hold fixed
                      start = mean(kyphosis$start))

# X beta
xb <- as.matrix(testage) %*% matrix(o$par, ncol = 1)
# inverse logit
y_hat <- exp(xb) / (1 + exp(xb))
plot(testage$age, y_hat, type = "l",
     main = "Probability of kyphosis",
     xlab = "Age (months)", ylab = "P")



# check against stats::glm
mod <- glm(present ~ age + number + start,
           family = binomial(link = "logit"),
           data = kyphosis)
summary(mod)

pred <- predict(mod, newdata = testage)
pred <- exp(pred) / (1 + exp(pred))
plot(testage$age, pred, type = "l",
     main = "Probability of kyphosis - stats::glm",
     xlab = "Age (months)", ylab = "P")


# compared with linear regression ("linear probability model")
# - note, becomes impossible for the model to be correct for large values
linmod <- glm(present ~ age + number + start,
              family = gaussian(link = "identity"),
              data = kyphosis)
linpred <- predict(linmod, newdata = testage)
plot(testage$age, linpred, type = "l",
     main = "Probability of kyphosis - linear regression",
     xlab = "Age (months)", ylab = "P")
```


## Example 2 - Poisson regression on the seatbelts dataset

Model: $Y|X \sim Poiss(\mu(X))$

We will use the canonical log link for the Poisson case.

The cumulant generating function is $b = e^\theta$.  

The log link function is simply $g = (b')^{-1} = \log(\theta)$.  

$\phi$ (the dispersion paramater) is also $1$ for the Poisson family.  


Thus, we have the simple log-likelihood:  
$$
l_n(\beta; Y, X) = \Sigma_i (~ Y_i ~ X_i^\top \beta - e^{X_i^\top \beta} ~)
$$


**DATA**:  

The seatbelts dataset contains information collected on monthly totals of car accidents in Great Britain from 1969 to 1984.  

Variables:  

- DriversKilled: count of drivers killed  
- kms: distance driven  
- PetrolPrice: petrol price  


```{r}
Seatbelts <- data.frame(Seatbelts)
head(Seatbelts)
xvars <- c("kms", "PetrolPrice")

# GLM (Poisson regression)
# Y|X ~ Poisson
# canonical log link
ll.poiss <- function(x, y) {
  nx <- ncol(x)
  x <- as.matrix(x, ncol = if (is.null(nx)) 1 else nx)
  x <- cbind(1, x) # intercept
  y <- as.matrix(y, ncol = 1)
  function(b) {
    b <- as.matrix(b, ncol = 1)
    xb <- x %*% b
    sum(y * xb - exp(xb))
  }
}

ll <- ll.poiss(x = Seatbelts[, xvars],
               y = Seatbelts[, "DriversKilled"] )

o <- optim(par = c(5, -0.3, -5), # initial param values
           fn = ll, # fn to optimize
           control = list(fnscale = -1)) # optim is a minimizer, we need max

# final param values
setNames(o$par, c("Intercept", xvars))

# compute on "new" data...
test <- data.frame(intercept = 1,
                   kms = mean(Seatbelts$kms), # hold fixed
                   PetrolPrice = seq(0, 0.5, length = 500)) 

# X beta
xb <- as.matrix(test) %*% matrix(o$par, ncol = 1)
# inverse link
y_hat <- exp(xb)
plot(test$PetrolPrice, y_hat, type = "l",
     main = "Number of drivers killed",
     xlab = "Petrol Price", ylab = "Count")


# check against stats::glm
mod <- glm(DriversKilled ~ kms + PetrolPrice,
           family = poisson(link = "log"),
           data = Seatbelts)
summary(mod)

pred <- predict(mod, newdata = test)
pred <- exp(pred) # inverse of logit
plot(test$PetrolPrice, pred, type = "l",
     main = "Number of drivers killed - stats::glm",
     xlab = "Petrol Price", ylab = "Count")


# compared to linear regression - stops making sense!
linmod <- lm(DriversKilled ~ kms + PetrolPrice,
             data = Seatbelts)

plot(test$PetrolPrice, predict(linmod, newdata = test), type = "l",
     main = "Number of drivers killed - linear regression",
     xlab = "Petrol Price", ylab = "Count")

```






```{r echo=FALSE, include=FALSE}
# model rate of capture of prey as a function of the density of the prey
n <- 1000
## alpha = maximum capture rate
alpha <- 0.2
## h = prey density at which capture rate is half the maximum rate
h <- 0.5

# observed
## prey density
x <- rbeta(n, shape1 = 2, shape2 = 3)
## rate of capture
u <- (alpha * x) / (h + x)
y <- rowMeans(replicate(n, rgamma(n, shape = u, scale = u)))

plot(x, y)

# Model:
# 1/y = (1/x)b + e
# 1/y = 1/alpha + (h/alpha) * (1/x)
# 1/y = b_0 + b_1 * (1/x)
#
# alpha_hat = 1 / b_0
# h_hat     = b_1 / h
# y_recip = 1/y
# x_recip = 1/x
mod <- glm(y ~ x, family = Gamma(link = "inverse"))
summary(mod)

pred <- predict(mod, newdata = data.frame(x=x))
plot(x, 1/pred,
     main = "glm - Gamma regression",
     xlab = "prey density",
     ylab = "predicted rate of capture") # or, plot(x, mod$fitted.values)

dispersion <- with(mod, sum((weights*residuals^2)[weights > 0]) / df.residual)

shape <- 1 / dispersion
scale <- 1 / (coef(mod)[1] + coef(mod)[2] * x) / shape



linmod <- lm(y ~ x)
plot(x, linmod$fitted.values,
     main = "linear regression",
     xlab = "prey density",
     ylab = "predicted rate of capture")
```







