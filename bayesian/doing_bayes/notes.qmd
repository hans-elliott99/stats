---
title: "Random notes from 'Doing Bayesian Data Analysis'"
author: "Hans Elliott"
format:
  html:
    embed-resources: true
toc: true
---


*Doing Bayesian Data Analysis*, Kruschke 2015  



# Basic probability and Bayes rule

The two-way distributions table:

Eye x Hair color | Black | Brunette | Red | Blond | Marginal (eye color)
--- | --- | --- | --- | --- | ---
Brown | $p(Brown, Black)$ | $p(Brown, Brunette)$ | $p(Brown, Red)$ | $p(Brown, Blond)$ | $p(Brown)$
Blue | $p(Blue, Black)$ | $p(Blue, Brunette)$ | $p(Blue, Red)$ | $p(Blue, Blond)$ | $p(Blue)$
Green | $p(Green, Black)$ | $p(Green, Brunette)$ | $p(Green, Red)$ | $p(Green, Blond)$ | $p(Green)$
Marginal (hair color) | $p(Black)$ | $p(Brunette)$ | $p(Red)$ | $p(Blond)$ | $1$



**Conditional probability example:**    

Eye x Hair color| Black | Brunette | Red | Blond | Marginal (eye color)
--- | --- | --- | --- | --- | ---
Brown | $\frac{p(Brown, Black)}{p(Brown)}$ | $\frac{p(Brown, Brunette)}{p(Brown)}$ | $\frac{p(Brown, Red)}{p(Brown)}$ | $\frac{p(Brown, Blond)}{p(Brown)}$ | $\frac{p(Brown)}{p(Brown)} = 1$


Note:  

- Each of the inner cells is a joint probability, or the probability of both events happening together - e.g., hair is black and eyes are brown.  
- Marginal probability is the probability of one of the variables while ignoring the other. Thus, it is recorded in the margin of the table, and is the sum across the rows or columns. E.g., $p(Brown) = p(Brown, Black) + p(Brown, Brunette) + p(Brown, Red) + p(Brown, Blond)$.  
- Conditional probability is the probability of one of the variables given the other variable's value is already set. So naturally, we look at the joint probability of the events happening together - e.g., hair is black and eyes are brown - and normalize by the base (marginal) probability of the event we are conditioning on - e.g., eyes are brown. So conditioning amounts to refocusing from the whole table to a specific row or column.    

- $p(a,b) = p(a | b) p(b) = p(b | a) p(a)$  
  - E.g., $p(\theta, Data) = p(\theta | Data)p(Data) = p(Data | \theta)p(\theta)$  
  $\Rightarrow p(\theta | Data) = \frac{p(\theta, Data)}{p(Data)} = \frac{p(Data | \theta) p(\theta)}{p(Data)}$. So this is how we get Bayes rule, or that "the conditional probability of the parameter given the data is equal to the conditional probability of the data given the parameter (likelihood) times the marginal probability of the parameter (prior) divided by the marginal probability of the data (evidence/normalizing constant).
  
- If $a$ and $b$ are independent events, then $p(a | b) = p(a)$ and $p(b | a) = p(b)$, and $p(a, b) = p(a)p(b)$.  


We can use the two-way table to think about Bayes' rule.  
Let the value of the paramater $\theta$ vary across the rows and the value of the $Data$ vary across the columns (imagine $\theta$ and $Data$ are discrete, for simplicity):  

$\theta$ x $Data$ | $Data_1$ | $\dots$ | Marginal ($\theta$)
--- | --- | --- | ---
$\theta_1$ |  $p(Data_1, \theta_1) = p(\theta_1 | Data_1)p(Data_1)$ | $\dots$ | $p(\theta_1)$
$\theta_2$ |  $p(Data_1, \theta_2) = p(\theta_2 | Data_1)p(Data_1)$ | $\dots$ | $p(\theta_2)$
$\dots$ | $\dots$ | $\dots$ | $\dots$
**Marginal ($Data$)** | $p(Data_1)$ | $\dots$ | $1$

In each cell we have the joint probability of a parameter value and data value occurring together. We know that Bayes' rule is $p(\theta | Data) = \frac{p(Data, \theta)}{p(Data)}$, so we can see that the numerator comes from these joint probabilities. We then normalize by the marginal probability of the data, which comes from the margin of the column of interest.  
Thus, when the parameter of interest is assigned to the rows of the table, Bayes' rule is the act of reassigning credibility from the row margins to a specific column.  
Without any $Data$, the best guess for $\theta$ comes from its marginal distribution, $p(\theta)$. But when we get some observation $Data_1$, we can redistribute credibility by focusing on the $Data_1$ column, where the joint probability for any $\theta_i$ is $p(Data_1, \theta_i) = p(\theta_i | Data_1) p(Data_1)$. By then dividing by $p(Data_1)$, we get an updated probability for $\theta_i$ conditional on the data we've received - $p(\theta_i | Data)$.  

To use the eye-color X hair-color example: without any knowledge of a person's hair color, our best guess for their eye color is the marginal distribution of eye color. But if we know their hair is black, then this changes the probabilities assigned to each eye color. We want to incorporate this new information by conditioning on it, and we compute $p(\text{Eye color} | \text{Black hair})$ using Bayes' rule.



## 2 Coins Example

An illustrative example from my probability class:  

We hold a bag that contains 2 types of coins - a *fair* coin which has probability of tails $p(tails) = 0.5$ and $p(heads) = 0.5)$, and a *biased* coin which has probability of tails $p(tails = 0.9)$ and $p(heads) = 0.1$. We know that $80\%$ of the coins in the bag are fair, and $20\%$ are biased.  
We take a random coin from the bag and flip it $10$ times (independent trials), observing $6$ tails (and $4$ heads).  
What is the probability that the coin is biased?

The question asks us to compute $p(\text{biased} | \text{observed 6 tails})$. So the parameter we are really interested in is *biased* versus *fair* - which can also be interpreted as determining which $p(tails)$ generated the data we observed (0.5 or 0.9?).    

By Bayes' rule,  
$$
p(\text{biased} | \text{observed 6 tails}) = \frac{p(\text{observed 6 tails} | \text{biased}) p(\text{biased})}{p(\text{observed 6 tails})}
$$

This is useful because, from the story, we know enough to compute all of these individual quantities:   

- We know a priori that the probability of drawing a biased coin is $p(\text{biased}) = 0.2$. The $p(\text{biased})$ term in the numerator is called the **prior**, because it encodes the prior information we have on the parameter of interest, before we see the data.  

- Next, we can easily compute the $p(\text{observed 5 tails} | \text{biased})$ by using the Binomial distribution.  
Since each coin flip is independent, the total number of tails observed can be modeled by a Binomial random variable with parameters $n = 10$ (total number of coin flips) and $p = 0.9$ (the probability of flipping a tails while using a biased coin).    
Using the probability mass function for the Binomial distribution, $p(\text{observed 6 tails} | \text{biased}) = \binom{10}{6} 0.9^6 0.1^4$:  


```{r}
choose(10, 6) * (0.9^6) * (0.1^4)
```

- We can use the same strategy to compute the probability of observing 6 tails given a fair coin, which we will need later. $p(\text{observed 6 tails} | \text{fair}) = \binom{10}{6} 0.5^6 0.5^4$:  

```{r}
choose(10, 6) * (0.5^6) * (0.5^4)
```
- Notice that the probability is much higher for the fair coin than the biased coin. This agrees with intuition, since seeing 6/10 tails seems fairly likely for a fair coin, and quite unlikely for the biased coin where we would expect 9/10 tails on average.  
Together, these 2 quantities represent values of the **likelihood** function, since we hold the data fixed and vary the parameter ($p$). The likelihood function tells us how likely it is to observe data given a specific parameter value. As we can see in this example, a parameter value that makes the data more likely will have a higher likelihood value.
    - The actual likelihood function could be written as $l(p | x) = {10 \choose 6} p^6 (1-p)^4$, where $x$ denotes "data", which is just the outcome of our coin flips in this case. Note that each of the above quantities are valid probabilities computed from valid probability distributions, but in general, the likelihood function is not a probability distribution. Since we hold the data fixed and vary the parameter, integrating (or summing up in the discrete case) the likelihood function will not necessarily sum to 1 (consider that the $Binomial(n, p)$ distribution sums to 1 when we sum over all its possible values, from $0$ to $n$, not all of the possible values of $p$, which range from $0$ to $1$ in general).


- Finally, the denominator calls for the marginal probability of observing 6 tails. This is not given to us by the story, and in practice would have to be computed by repeatedly flipping coins of both varieties while ignoring their biasedness, and then computing the proportion of tails. Computing this requires us to marginalize over (sum over) the two types of coins, which effectively computes the probability of observing 6 tails while ignoring the biasedness of the coin. We can use the law of total probability here because we know that the types of coins partition our sample space - we either draw a fair coin or a biased coin, but not both. Thus:  
$$
p(\text{observed 6 tails}) = p(\text{6 tails} | \text{fair}) p(\text{fair}) + p(\text{6 tails} | \text{biased}) p(\text{biased})
$$
    - The law of total probability states that if $B_1$ and $B_2$ are events that partition the sample space, then $P(A) = P(A \cap B_1) + P(A \cap B_2)$ (draw a Venn diagram to see). Then consider that $P(A | B_i) = \frac{P(A \cap B_i)}{p(B_i)}$, so $P(A) = P(A | B_1)P(B_1) + P(A | B_2)P(B_2)$. 


Now we have everything we need to compute the probability that the coin is biased, given our observation of 6 tails, which is known as the **posterior probability**:  

$$
p(\text{biased} | \text{6 tails}) = \frac{p(\text{6 tails} | \text{biased}) p(\text{biased})}{p(\text{6 tails} | \text{biased}) p(\text{biased}) + p(\text{6 tails} | \text{fair})p(\text{fair})} = \frac{0.011\cdot0.2}{0.011 \cdot 0.2 + 0.205 \cdot 0.8}
$$
```{r}
#| code-fold: true
two_coins_posterior <- function(
        prior_fair = 0.8,
        prob_tails_fair = 0.5,
        prob_tails_biased = 0.9,
        num_flips = 10,
        num_tails = 6
) {
    prior_biased <- 1 - prior_fair
    likelihood_biased <- choose(num_flips, num_tails) *
        (prob_tails_biased^num_tails) *
        (1 - prob_tails_biased)^(num_flips - num_tails)
    likelihood_fair <- choose(num_flips, num_tails) *
        (prob_tails_fair^num_tails) *
        (1 - prob_tails_fair)^(num_flips - num_tails)
    
    posterior <- (likelihood_biased * prior_biased) /
      ((likelihood_biased * prior_biased) + (likelihood_fair * prior_fair))
    
    return(posterior)
}
```

```{r}
two_coins_posterior(
    prior_fair = 0.8,
    prob_tails_fair = 0.5,
    prob_tails_biased = 0.9,
    num_flips = 10,
    num_tails = 6
)
```


We can see there is about a $1.3\%$ chance that the coin is biased, given our observation of 6 tails. This is quite small since 6 tails would be a fairly low number for a coin with such an extreme bias.  

We can vary some of the parameters of the experiment to see how it affects the posterior probability.  

For example, if the biased coin is only slightly biased, the posterior probability increases quite a bit.  

```{r}
two_coins_posterior(
    prior_fair = 0.8,
    prob_tails_fair = 0.5,
    prob_tails_biased = 0.6,
    num_flips = 10,
    num_tails = 6
)
```

If we now increase the quantity of data, but maintain the 6/10 ratio, the posterior probability becomes quite large. This is intuitive, as now the data strongly suggests that the underlying probability of tails is 0.6, which agrees with the biased coin.  

```{r}
two_coins_posterior(
    prior_fair = 0.8,
    prob_tails_biased = 0.6,
    num_flips = 100,
    num_tails = 60
)
```


I like this fairly simple example because it is enough to start to imagine how Bayes' rule can be applied to scientific problems.  
Here our parameter of interest is the "biasedness of the coin", and by recording flips we've collected data to help to inform our belief about this parameter, which directly mirrors the scientific process! In practice, we'd be interested in the actual parameter $p$ of the Binomial distribution, which we would assign some scientific interpretation (e.g., if $p$ is the probability of testing positive for a disease). Thus we would consider a whole range of $p$ values, which would make the computation of the denominator more complicated (we have to marginalize out $p$, which is continuous on $[0, 1]$).  

The analogy goes even further though. In order to compute the posterior, we had to make some assumptions about the data (we assumed each coin flip is independent) so that we could model the data using some known distribution (the Binomial) for which we can calculate the probability density/mass function (and thus the likelihood). This is often how scientific questions are approached. We also had to specify a prior probability on the parameter of interest, reflecting what we know about the coin a priori. In Bayesian analysis, a prior is always required.  
Computing the denominator required some more work, which is fitting since this is historically what made Bayesian analysis difficult. For higher dimensional problems, it becomes difficult or impossible to compute the denominator, and we must rely on other tools like Markov Chain Monte Carlo methods to compute the posterior with just the likelihood and prior.  




# MCMC Intuition

In Bayesian analysis we use Bayes' rule to compute the posterior probability over the parameter space $\theta$ given the data. We are primarily interested in obtaining an estimate for $\theta$, but we rely on Bayes' rule because it allows us to produce a $p(\theta | Data)$ from a $p(Data | \theta)$, or likelihood, and $p(\theta)$, or prior, and the likelihood and prior are both objects that we can define and calculate at the start of a problem.  
To be a valid probability distribution, the posterior must integrate to 1, so the denominator in Bayes' rule is needed to normalize the numerator. From Bayes' rule we know the denominator is $p(Data)$, which is the marginal distribution of the data, and thus can be rewritten as the marginalizing out of $\theta$ from the joint distribution, or the numerator: $\int_{\theta \in \Theta} p(Data | \theta) p(\theta) d\theta$. So after we've defined the likelihood and prior, we really need to calculate this integral as a final step.  

We use MCMC because it is often very difficult or impossible to analytically evaluate this integral. So even though we can compute the numerator, if we can't normalize it, we don't have a valid probability distribution.     

MCMC methods work around this by working directly with the unnormalized posterior, $p(Data | \theta) p(\theta)$, as the *target distribution*, and finding clever ways to sample from it - i.e., generate random values of $\theta$ - that represent the actual posterior distribution.  


## Metropolis Algorithm

We have some target distribution $P(\theta)$ over a multi-dimensional continuous parameter space. We must be able to compute the value of $P(\theta)$ for any candidate value of $\theta$. But, $P(\theta)$ does not have to be normalized, merely nonnegative.  

Sample values from $P(\theta)$ are generated by taking a random walk through the parameter space.  
The walk begins at some arbitrary, specified point, where $P(\theta)$ is nonzero.  

At each time step, a proposal is made for a move to a new position in parameter space. The proposal comes from a *proposal distribution*, which can take many different forms and can be tuned for efficiency.  
The proposal value for $\theta$ is either accepted or rejected. If rejected, the current value of $\theta$ is retained. The proposal is accepted with probability $p_{move} = \frac{P(\theta_{proposed})}{P(\theta_{current})}$ (if $p_{move} > 1$, the move is always accepted else it is accepted probabilistically by simulating a random uniform deviate for comparison).  

This works out because the relative transition probabilities between positions exactly match the relative values of the target distribution, such that each position in parameter space will be visited proportionally to its target value.  

The sequence of samples is a Markov chain because at each step, the distribution over the next sample depends only on the value of the current sample, and not on any previous samples.

Wikipedia has a nice intuitive explanation: [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)


### Example - Metropolis algorithm for Binomial likelihood with Beta prior

Binomial PMF:  
$f(\theta) = {N \choose z} \theta^z (1 - \theta)^{N - z}$, where $\theta \in \{0, 1\}$  

(Note we can drop the binomial coefficient since it does not depend on $\theta$).  

Beta PDF, as a prior:  
$f(\theta) = \frac{\theta^{\alpha - 1}(1-\theta)^{\beta - 1}}{B(\alpha, \beta)}$, where $x \in [0, 1]$, $\alpha, \beta > 0$, and $B$ is the Beta function.  



```{r}
#| fig.width: 6
#| fig.height: 6
alphas <- c(1, 2, 3)
betas <- c(1, 2, 3)
thetas <- seq(0, 1, 0.01)

par(mfrow = c(length(alphas), length(betas)))

for (i in 1:length(alphas)) {
  for (j in 1:length(betas)) {
    plot(thetas, dbeta(thetas, alphas[i], betas[j]),
         type = "l",
         main = paste("Beta(", alphas[i], ",", betas[j], ")"),
         xlab = "Theta", ylab = "Density")
  }
}
```


Note that the Beta distribution is conjugate to the Binomial, so we can easily compute the posterior distribution analytically. The posterior will also be a Beta distribution:

$p(\theta | D) = Beta(\alpha + z, \beta + N - z)$



To calculate the acceptance probability:  

$$
p_{move} = \frac{P(\theta_{pro})}{P(\theta_{cur})} = \frac{p(D|\theta_{pro})p(\theta_{pro})}{p(D|\theta_{cur})p(\theta_{cur})} = \frac{Binomial(D|\theta_{pro})Beta(\theta_{pro}|\alpha, \beta)}{Binomial(D|\theta_{cur})Beta(\theta_{cur}|\alpha, \beta)}
$$





```{r}
# Example of Metropolis algorithm for a Binomial likelihood with Beta prior
run_metropolis_mcmc <- function(data,
                                target_fn,
                                proposal_fn,
                                theta_init,
                                niter = 1000,
                                warmup = floor(niter / 2)) {

  message("* Running MCMC - Metropolis algorithm")
  # Initialize
  t0 <- Sys.time()
  chain <- numeric(niter)
  theta_cur <- theta_init
  # Sample
  n_acc <- 0 # track number of accepted proposals
  for (iter in seq(1, warmup + length(chain))) {
    # 1. Generate proposal value
    theta_pro <- theta_cur + proposal_fn()
    # 2. Calculate acceptance probability
    p_move <- target_fn(theta_pro, data) / target_fn(theta_cur, data)
    # 3. Accept or reject
    if (p_move > 1 || runif(1) < p_move) {
      theta_cur <- theta_pro
      if (iter > warmup) n_acc <- n_acc + 1
    }
    # 4. Store sample
    if (iter > warmup) {
      chain[iter - warmup] <- theta_cur
    }
    # Repeat ...
  }
  message("** Finished in ",
          round(difftime(Sys.time(), t0, units = "secs"), 1), "s")
  message("** Acceptance rate: ", n_acc / length(chain))
  return(chain)
}


# Binomial likelihood
# N flips, z heads, governed by prob. of head theta
.bin_lik <- function(theta, N, z) {
  return( theta^z * (1 - theta)^(N - z) )
}
# Beta prior
# theta in (0, 1), alpha, beta > 0
.beta_prior <- function(theta, alpha, beta) {
  return( dbeta(theta, alpha, beta) )
}

# Specifies the unnormalized posterior distribution
target_relative <- function(theta, D) {
  return(.bin_lik(theta, D$N, D$z) * .beta_prior(theta, D$alpha, D$beta))
}

# Specifies the proposal distribution - SD can be tuned
proposal_jump <- function() {
  return(rnorm(1, mean = 0, sd = 0.5))
}


# Simulate some data
## We observe the number of heads, z, out of N coin flips,
## and we control the underlying probability of heads (theta).
## Also specify values for prior dist. parameters, alpha and beta.
##
## Fun to mess with the number of flips ("amount of data") and the priors.
## A very flat prior with not many flips may lead to a posterior that seems
## quite different from the underlying simulation, since z/N may be far from p.
set.seed(1614)
D <- within(list(), {
  # data gen process
  N <- 10
  p <- 0.66
  z <- rbinom(1, size = N, prob = p)
  # prior dist params
  alpha <- 2
  beta <- 2
})



chain <- run_metropolis_mcmc(D, target_relative, proposal_jump,
                             theta_init = 0.5,
                             niter = 10000)

```


```{r}
# for visualizations, take advantage of simple nature of this problem to derive
# posterior dist. and mean analytically
true_posterior <- function(theta, D) {
  return(dbeta(theta, D$alpha + D$z, D$beta + D$N - D$z))
}
true_posterior_mean <- function(D) {
  a <- D$alpha + D$z
  b <- D$beta + D$N - D$z
  return(a / (a + b))
}


#
# trace plot
#
plot(seq_along(chain), chain, type = "l",
     main = "Traceplot", xlab = "Iteration", ylab = "Theta")
abline(h = D$p, col = "blue")
abline(h = true_posterior_mean(D), col = "red")
legend("topright",
       legend = c("True value", "True Post. mean"),
       lty = 1,
       col = c("blue", "red"))


#
# histogram
#
hist(chain, breaks = 20, main = "Posterior", xlab = "Theta")
abline(v = D$p, col = "blue")
abline(v = mean(chain), col = "black")
abline(v = true_posterior_mean(D), col = "red", lty = 2)
legend("topright", legend = c("True value", "Posterior mean", "True Post. mean"),
       col = c("blue", "black", "red"), lty = c(1, 1, 2))


#
# density
#
thetas <- seq(0, 1, 0.01)
plot(thetas, true_posterior(thetas, D), type = "l", col = "red",
     main = "True posterior", xlab = "Theta", ylab = "Density")
lines(density(chain), col = "black", lty = 2)
legend("topleft", legend = c("True posterior", "Estimated posterior"),
       col = c("red", "black"), lty = 1)



```


One problem with the Metropolis algorithm is that the proposal distribution must be properly tuned to the posterior for the algorithm to work well. If too narrow or too wide, a large proportion of proposals will be rejected and the algorithm will get stuck sampling in a small region of the parameter space.  


## Other MCMC Methods

### Gibbs Sampling

Gibbs sampling is also a type of random walk through parameter space,
like the Metropolis algorithm. The walk starts at some arbitrary point, and at each point
in the walk, the next step depends only on the current position, and on no previous
positions. The key difference is in how the next position is chosen.  

Gibbs sampling is especially useful with multiple parameters. At each point in the walk, one of the component parameters from $\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ is selected, say $\theta_i$. Gibbs sampling generates a new value for $\theta_i$ by sampling from the conditional distribution of $\theta_i$ given all the other parameters and the data: $p(\theta_i | \theta_{j \ne i}, D)$. This is done for each parameter in turn, cycling through the parameters until all have been updated.

This can be thought of as a special case of the Metropolis algorithm where the proposal distribution depends on the location in the parameter space and the component paramater selected - the proposal distribution is the conditional posterior probability of that parameter.  
Because the proposal distribution exactly mirrors the posterior probability for that parameter, the proposed move is always accepted.

To accomplish Gibbs sampling, we must determine the conditional posterior distribution for each parameter.  


# MCMC Representativeness and Accuracy

Goals of generating MCMC samples:  

1. Values of the chain must be representative of the posterior distribution.  
  - Not influenced by the arbitrary initial value of the chain
  - Should fully explore the range of the posterior distribution without getting stuck  

2. The chain should be large enough that estimates are accurate and stable.  

3. The chain should be generated efficiently, with as few steps as possible.  


## Checks for unrepresentativeness

Via visual examination and numerical descriptions of convergence.  


```{r}
# example chain from above
chains <- lapply(
  1:4,
  function(x) run_metropolis_mcmc(D, target_relative, proposal_jump,
                                 theta_init = 0.5,
                                 niter = 10000)
)
```

### Trace plot: visual examination of chain's trajectory

Especially useful when you have multiple chains. If all the chains are representative, they should overlap.

Note, the preliminary steps, during which the chain moves from its unrepresentative initial value to the modal region of the posterior, is called the burn-in or warmup period, which is typically discarded.

```{r}
plot(seq_along(chains[[1]]), chains[[1]], type = "l", lty = 2, col = 1,
     main = "Traceplot", xlab = "Iteration", ylab = "Theta")
for (i in 2:4) {
  lines(seq_along(chains[[i]]), chains[[i]], col = i, lty = 2)
}
legend("topright", title = "Chain", legend = 1:4, col = 1:4, lty = 2)
```

### Density overlay: visual examination of chain's distribution

Again, looking for overlapping/similar shaped chains with stability in central and tail estimates.

```{r}
densities <- lapply(chains, density)
plot(densities[[1]], type = "l", col = 1, lty = 2,
     main = "Density overlay", sub = "With median & 95% HDI",
     xlab = "Theta", ylab = "Density")
for (i in 2:4) {
  lines(densities[[i]], col = i, lty = 2)
}
for (i in 1:4) {
  points(x = quantile(chains[[i]], c(0.025, 0.5, 0.975)), y = c(0,0,0),
         col = i, pch = "|", lheight = 0.25)
}
```

### Gelman-Rubin statistic/shrink-factor

Numerical measure of convergence. Compares the variance of the samples within each chain to the variance of the samples between chains.
Its value is 1.0 if the chains are fully converged, but its value is larger
than 1.0 if there are orphaned or stuck chains. As a heuristic, if the Gelman-Rubin
statistic is greater than 1.1 or so, you should worry that perhaps the chains have not
converged adequately.  

```{r}
# https://en.wikipedia.org/wiki/Gelman-Rubin_statistic
gelman_rubin <- function(chains) {
  J <- length(chains)
  L <- length(chains[[1]])
  # between chain variance
  chain_mean <- vapply(chains, mean, numeric(1)) 
  B <- (L / (J - 1)) * sum((chain_mean - mean(chain_mean))^2)
  # within chain variance
  W <- (1 / J) * sum(vapply(chains, function(x) {
    sum((x - mean(x))^2)
  }, numeric(1)))
  # estimate
  R <- ( ((L - 1) / L) * W + (B / L) ) / W
  return(R)
}

gelman_rubin(chains)
```

## MCMC Accuracy

Successive steps in a clumpy chain do not provide independent information about the parameter distribution, so we need measures of autocorrelation - the correlation of the chain values with itself ar $k$ steps ahead.  

### Autocorrelation function plots

We want to see the autocorrelation drop off very quickly to zero as $k$ (lag) increases, else the parameter values at successive steps in the chain are not providing independent information about the posterior, since each successive step is partially redundant with the previous step.  

```{r}
par(mfrow = c(2, 2))
acf(chains[[1]], lag.max = 100, main = "Chain 1")
acf(chains[[2]], lag.max = 100, main = "Chain 2")
acf(chains[[3]], lag.max = 100, main = "Chain 3")
acf(chains[[4]], lag.max = 100, main = "Chain 4")

```

### Effective sample size

The notion that not every sample may be contributing independent information to the posterior distribution tells us that we want some way to quantify how much independent information *is* being provided by a chain.  
This is the motivation for the effective sample size (ESS), which divides the actual sample size by the amount of autocorrelation:  

$$
ESS = \frac{N}{1 + 2 \sum_{k=1}^{\infty} ACF(k)}
$$
Where $ACF(K)$ is the autocorrelation of the chain at lag $k$. For practical computation, the sum is stopped at the $k$ where $ACF(k) \le 0.05$, for example. 

```{r}
ess <- function(chain, max_try = 3) {
  N <- length(chain)
  acf_vals <- acf(chain, plot = FALSE, lag.max = 100)$acf
  # if 100 isn't enough lags, try more
  ntry <- 0
  while (acf_vals[length(acf_vals)] > 0.05 && ntry < max_try) {
    ntry <- ntry + 1
    acf_vals <- acf(chain, plot = FALSE, lag.max = 100 + ntry * 50)$acf
  }
  ESS <- N / (1 + 2 * sum(acf_vals))
  return(ESS)
}

# number of iterations
cat(lengths(chains))
# effective sample size
cat(ess(chains[[1]]), ess(chains[[2]]), ess(chains[[3]]), ess(chains[[4]]))
```
More: <https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html>  

### MC Standard Error

We can use ESS to compute a Monte Carlo standard error (MCSE) for the mean of the chain.  
Recall the standard error of a statistic is the standard deviation of its sampling distribution, and the SE of the sample mean of $N$ i.i.d. observations is calculated as $SE = SD / \sqrt{N}$, where $SD$ is the standard deviation of the sample.  
The MCSE is very similar: $MCSE = SD / \sqrt{ESS}$, where $SD$ is the standard deviation of the chain and $ESS$ is the effective sample size.  

```{r}
mcse <- function(chain) {
  sd_chain <- sd(chain)
  ESS <- ess(chain)
  MCSE <- sd_chain / sqrt(ESS)
  return(MCSE)
}

sapply(chains, mcse)
```



## MCMC Efficiency

Run chains in parallel.  

Adjust sampling method based on problem (in practice, software implements for you).  

Re-parameterize the model if possible and helpful. (And at a minimum, mean-center predictors in regression.)  









