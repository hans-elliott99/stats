---
title: "Generalised Linear Models"
author: Hans Elliott
format:
  html:
    embed-resources: true
toc: true
---

```{r, warning=FALSE, message=FALSE, echo=TRUE}
set.seed(1614)

library(ggplot2)
```


In the previous GLM examples we derived log-likelihood functions in
terms of $\beta$ for the desired exponential family distributions, and
then manually optimized the log-likelihood using R's `stats::optim`
function.\
`optim` uses several possible general-purpose optimization methods to
optimize an input function. While this works, it's not how GLMs are
typically fit.

In practice GLMs are fit using Iteratively Re-Weighted Least Squares,
which is a convenient way to perform Netwon-Raphson or Fisher-scoring
updating by reformulating the update procedure as a weighted least
squares problem (see lecture notes 10b).


## GLM Summary

Quick recap of GLMs plus some notation:  

We assume $Y|X$ is distributed according to some sub-family of probability distributions from the greater exponential family (e.g., Gaussian, Poisson, binomial), with mean parameter $\mu = E[Y|X]$ and dispersion parameter $\phi$, which may be know.  


Distributions in the exponential family have density functions of the form (when written in canonical form):

$$
f_\theta(y) = \exp(~~ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) ~~)
$$
Where $b$ is known and is the cumulant generating function, $c$ is known, and is essentially a normalizing constant that we will drop when we maximize the likelihood, and typically $a(\phi) = \phi/w$, for some prior weights $w$ (which you can think of as $1$ in many common cases).  


Then we assume our model is of the form:
$$
g(\mu) = X \beta; ~~X\in\mathbb{R}^{n \times p}, \beta \in \mathbb{R}^p
$$

$g$ is the link function which maps the mean parameter $\mu$ from its domain to $\mathbb{R}$, so that it can be modeled by $X \beta$. The link must be invertible so that we can compute $\mu = g^{-1}(X \beta)$.  

We may refer to $\eta = X \beta$ as the "linear predictor".  

Also important to note are the following results:  

- $E[Y|X] = b'(\theta) = \mu$
- $Var(Y|X) = a(\phi) b''(\theta) = a(\phi) V(\mu)$, where $V(\mu)$ is known as the variance function and it describes how the variance depends on the mean.


Assuming $n$ i.i.d. observations we can write the likelihood for an exponential family member as the joint density function, or the product of $n$ probability density/mass functions ($L_n(\theta; y) = \prod_{i=1}^{n} f(y; \theta)$), or the log likelihood as the sum of the logs of the $n$ probability density/mass functions ($\ell_n(\theta; y) = \sum_{i=1}^n \log f(y; \theta)$). Maximizing the log-likelihood with respect to $\beta$, given the data we have, will give us estimates $\hat{\beta}$.  

Next we will explore how GLMs are fit, and then delve into model diagnostics and inference.



## Newton-Raphson / Fisher-Scoring Optimization Method

The Newton-Raphson optimization method updates the parameter estimate
iteratively, maximizing the log likelihood by performing a modified
gradient ascent. At each iteration $k$ we compute
$\beta^{(k + 1)} = \beta^{(k)} + [-H_{\ell_n}(\beta^{(k)})]^{-1} \nabla_{\ell_n}(\beta^{(k)})$

$\ell_n$ is the log-likelihood as a function of $\beta$, $\nabla$ is the
gradient of the log-likelihood, and $H$ is the Hessian.

The Fisher-scoring method uses the relation $I(\beta) = -E[H_{\ell_n}(\beta)]$, where $I(\beta)$ is the Fisher information matrix of $\beta$. Thus we replace the $-H_{\ell_n}(\beta^{(k)})^{-1}$ term with $I(\beta^{(k)})^{-1}$, which can sometimes be more convenient to compute.  

At each iteration $k$ we compute $\beta^{(k + 1)} = \beta^{(k)} + I(\beta^{(k)})^{-1} \nabla_{\ell_n}(\beta^{(k)})$

### Example: Bernoulli

For the Bernoulli family, we can derive (see lecture notes):

-   $\nabla(\beta) = X^\top (\frac{e^{X \beta}}{1 + e^{X \beta}} - Y)$

-   $-H(\beta) = I(\beta) = - X^\top diag(\frac{e^{X \beta}}{(1 + e^{X \beta})^2}) X$

    -   (the Hessian and Fisher-information matrix are equivalent up to the sign, since the canonical link is used)

```{r}
###
### newton-raphson / fisher-scoring
###
# sim data
nobs <- 1e3
xraw <- cbind(rnorm(nobs), rnorm(nobs))
p_true <- 1 / (1 + exp(-1)*exp(-2 * xraw[,1])*exp(-3 * xraw[,2]))
yraw <- rbinom(nobs, size = 1, prob = p_true)
# yraw <- p_true ## test to verify coefs are exact

### prep inputs
x <- cbind(1, as.matrix(xraw))
y <- as.matrix(as.numeric(yraw), ncol = 1)

# init beta
beta <- rep(0, ncol(x))

maxit <- 10
for (it in 1:maxit) {
  ###
  ### newton-raphson / fisher-scoring
  ###
  xb <- x %*% beta
  grad <- t(x) %*% (exp(xb) / (1 + exp(xb)) - y)
  hess <- - t(x) %*% (diag(as.numeric(exp(xb) / (1 + exp(xb))^2)))  %*% x

  betaold <- beta
  beta <- betaold + solve(hess) %*% grad
  # convergence?
  diff <- sqrt(sum((beta - betaold)^2)) # l2 norm, ie norm(x, type = "2")
  cat(paste0("[",it,"/",maxit,"]: l2 norm = ", diff, "\n"))
}


cat("Our Newton-Raphson coefficients:\n", beta, "\n")

fitglm <- glm(y ~ xraw, family = binomial(link = "logit"))
cat("stats::glm coefficients:\n", coef(fitglm), "\n")

```

## Iteratively Reweighted Least Squares

Using the particular functional form of distributions in the exponential
family, we are able to rewrite the Fisher-scoring method as a weighted
least squares problem (derivation in notes).

The IRLS update procedure used to obtain the $k + 1$-th iteration's
coefficient can be written as:

$$\beta^{(k + 1)} = ({X}^\top {W} {X})^{-1} {X}^\top {W}(\tilde{{Y}} - \tilde{\mu} + X\beta^{(k)})$$
where:

-   $W = diag(\frac{h'(X_i^\top \beta)}{g'(\mu_i)a(\phi)})$
-   $\tilde{Y}_i = Y_i ~ g'(\mu_i)$ and
    $\tilde{\mu}_i = \mu_i ~ g'(\mu_i)$
-   $g$ is the link function, such that $\mu_i = g^{-1}(X^\top \beta)$
-   $h = (b')^{-1} \circ g^{-1}$, given the cumulant generating function
    $b$ ($h$ is identity under the canonical link).

A **Weighed Least Squares** problem is concerned with deriving the
maximum likelihood estimator by solving:
$\min_\beta (Y - {X}\beta)^\top {W} (Y - {X}\beta)$, which has a
closed-form solution: $$
\hat{\beta} = ({X}^\top {W} {X})^{-1} {X}^\top {W} Y
$$

This matches the update procedure formula above when:

1.  $W = W(\beta^{(k)})$ is the weight matrix
2.  $Y = \tilde{{Y}} - \tilde{\mu} + {X}\beta^{(k)}$ is the response
    (below called $Z$)

We can also reformulate the working weight matrix and working response
vector to utilize more common functions (see notes for math).

$${W}^{(k)}= diag \frac{w ~ \mu'(\eta)^2}{V(\mu)}$$
$$Z_i^{(k)} = \eta + \frac{(Y_i - \mu_i^{(k)})}{\mu'(\eta)}$$

To compute these we need the following functions and variables, which are dependent on the particular exponential family-member we are fitting a GLM for:  

-   $\partial \mu / \partial X\beta$, where $X \beta$ is often written as $\eta$ in the GLM world, so we need $\mu'(\eta)$ (recall $\mu = g^{-1}(\eta)$, so $\mu' = (g^{-1})'$). This is the function that `stats::glm` refers to as `mu.eta`.
-   $V(\mu)$, the variance for the exponential family as a function of $\mu$
- $w$, the prior weight vector (recall $a(\phi) = \phi/w$ from above)


**Algorithm**:\
Given a $\beta^{(k)}$ at step $k$, we can obtain $\beta^{(k+1)}$ by the
following:\
1. Fix $\beta^{(k)}$ and set:
$$\mu_i^{(k)} = g^{-1}(X_i^\top \beta^{(k)})$$ 2. Calculate the adjusted
dependent responses $Z_i^{(k)}$\
3. Compute the weights ${W}^{(k)} = {W}(\beta^{(k)})$\
4. Regress (i.e., WLS) ${Z}^{(k)}$ on the design matrix ${X}$ with
weight ${W}^{(k)}$ to derive a new estimate:\
$$\beta^{(k + 1)} = ({X}^\top {W}^{(k)} {X})^{-1} {X}^\top {W^{(k)}} Z^{(k)}$$
5. Repeat until convergence.

For this procedure we need:

- ${X}$ and ${Y}$ (the data) and initial values of $\beta$
- The functions and variables needed for the working weight matrix and working response, listed above: $\mu'(\eta)$, $V(\mu)$, and $w$

## Weighted Least Squares

Since we are going to need WLS for the IRLS procedure, we can implement
that first.

Above we saw that a WLS has the solution:\
$$\hat{\beta} = ({X}^\top {W} {X})^{-1} {X}^\top {W} Y$$ However, note
that we can rewrite the weight matrix as $W = \sqrt{W} \sqrt{W}$ because
it is positive semidefinite (it is a diagonal matrix and has only
positive entries on the diagonal), which just means that it's valid for
us to write the above identity (since
$x^\top W x \ge 0 , ~~\forall x \in R$).

Thus, we can re-write this as:

$$\hat{\beta} = (\sqrt{W}{X}^\top \sqrt{W}{X})^{-1} \sqrt{W}{X}^\top \sqrt{W} Y$$

Which is the same solution as typical linear regression
($\hat{\beta} = ({X}^\top {X})^{-1} {X}^\top Y$) where the response
variable and design matrix are pre-multiplied by $\sqrt{W}$.

(This is mainly useful because it allows us to re-use statistical
routines - for example, we could essentially do a `lm` regression of
`(sqrt(w) * y) ~ (sqrt(w) * x)`, see below...)

```{r}
wls <- function(y, x, w = rep_len(1L, nrow(x))) {
  # y = Response vector
  # x = Design matrix
  # w = Weights vector - i.e., diag of weights matrix.
  #     Default is all 1s, equivalent to linear regression.
  y <- as.matrix(y, ncol = 1)
  x <- as.matrix(x)
  if (is.matrix(w)) stop("Weights must be a vector.")
  if (any(w < 0)) stop("Weights must be positive.")
  sqw <- sqrt(w)
  xw <- x*sqw
  yw <- y*sqw
  # do solve(t(xw) %*% xw) %*% t(xw) %*% yw, but below should be faster
  betas <- crossprod(solve(crossprod(xw)), crossprod(xw, yw))
  return(c(betas))
}

y <- rnorm(100)
x <- cbind(1, rnorm(100), rnorm(100))
w <- with(list(x = rbinom(100, 1, 0.5)), {x / sum(x)})

cat("Our wls coefs:\n", wls(y, x, w))

cat("stats::lm.wfit coefs:\n", coef(lm.wfit(y = y, x = x, w = w)))

yw <- sqrt(w) * y
xw <- sqrt(w) * x
cat("stats::lm with y and x pre-multiplied by sqrt(w) coefs:\n",
    coef(lm(yw ~ -1 + xw)), "\n")
```

```{r, echo=FALSE}
rm(y,x,w,yw,xw)
```



## GLM by IRLS Implementation

Now we can implement the IRLS algorithm.

The function takes in a model/design matrix, the response vector, and a "fam" object which is just a bare-bones implementation of `stats::family` objects (examples to come). The "fam" object contains the family-specific functions needed in the IRLS steps, namely the link function $g$, its inverse $g^{-1}$, the derivative of $\mu$ with respect to the linear predictor $\mu'(\eta)$, and the variance function $V(\mu)$.


I also use an initializer function attached to the "fam" instance
(`fam$init`), which uses initialization techniques borrowed from
`stats::glm` and `stats::family` to get good starting values for $\beta$ (and technically $\mu$).
In my experience, it is important to initialize well or the algorithm may not converge.
It follows the starting method similair to the one given by the original paper from Nelder and Wedderburn, 1972, and used by R:  

- Set the initial $\mu$ equal to the data, $y$, since that is a reasonable initial estimate for $\mu = E[Y | X]$ 
  - Although, $y$ may need to be modified depending on the family. "For instance, with the binomial distribution it will probably be adequate to replace instances of $y = 0$ or $y = n$ with $y = 1/2$ and $y = n - 1/2$, where, e.g. with the probit and logit transformations, $\mu = 0$ or $\mu = n$ would lead to infinite values" (N & W, 1972)
- Calculate $\eta$ from initial $\mu$ via $g$, the link function, since $\eta = g(\mu)$.  


Finally, after the IRLS algorithm estimates the coefficients, we perform a few more steps to caclulate goodness of fit statistics and do some inference. Those steps will be explained below and can be ignored for now.

```{r}
fit_glm_irls <- function(X,                # design/model matrix, n x p
                         y,                # response vector, n x 1
                         fam,              # exponential family-member object
                         weights = 1,      # prior weights, length 1 or n
                         tol = 1e-8,       # IRLS stopping tolerance
                         maxit = 25,       # maximum IRLS iterations
                         verbose = TRUE    # print IRLS iteration updates?
                         ) {
  xnms <- names(X)
  xnms[1] <- "Intercept"
  x <- as.matrix(X)
  y <- as.matrix(as.numeric(y), ncol = 1)
  if (length(weights) == 1) weights <- rep_len(weights, nrow(x))
  nobs <- nrow(x)
  rank <- ncol(x)
  #
  # IRLS Iterations
  #
  # initialize mu & update y and weights if needed
  i <- fam$init(y = y, weights = weights, fam = fam)
  mustart <- i$mustart
  y       <- i$y
  weights <- i$weights
  beta    <- rep_len(0, rank)
  for (it in 1:maxit) {
    eta <- if (it == 1) fam$link(mustart) else x %*% beta  # Xb
    mu  <- if (it == 1) mustart else fam$link_inv(eta) # g(mu) = Xb
    muetaval <- fam$mu_eta(eta)  # deriv of mu wrt to eta
    z <- eta + (y - mu) / muetaval  # working response vec
    w <- c((weights * muetaval^2) / fam$variance(mu))  # weight diag
    betaold <- beta
    beta <- wls(y = z, x = x, w = w)
    # are we there yet?
    diff <- sqrt(crossprod(beta - betaold)) # l2 norm
    if (verbose) cat(paste0("[",it,"/",maxit,"]: l2 norm = ", diff, "\n"))
    if (diff < tol) break
  }
  mu_final <- c(fam$link_inv(x %*% beta)) # final fitted values
  #
  # deviance
  #
  # residual deviance
  dev_vec <- fam$deviances(y, mu_final, weights)
  dev <- sum(dev_vec)
  # null deviance - i.e., that of intercept only model
  wtdmu <- sum(weights * y) / sum(weights)
  dev_null <- sum(fam$deviances(y, wtdmu, weights))
  #
  # residuals
  #
  r_response <- c(y - mu_final) # response residual
  r_pearson <- c(r_response / sqrt(fam$variance(mu_final)))
  r_deviance <- c(sign(r_response) * sqrt(dev_vec))
  #
  # dispersion
  #
  if (fam$name_family %in% c("binomial", "poisson")) {
    phi <- 1
  } else {
    phi <- sum(weights * (r_pearson)^2) / (nobs - rank)
  }
  #
  # variance-covariance matrix
  #
  vc <- phi * solve(crossprod(sqrt(w) * x))
  se <- sqrt(diag(vc))
  #
  # coef. sig. tests
  #
  # h0: b_j == 0; h1: b_j != 0
  tstat <- beta / se
  if (fam$name_family %in% c("binomial", "poisson")) {
    ttype <- "z" # since phi is 1
    pval <- 2 * pnorm(-abs(tstat)) # prob more extreme than tstat
  } else {
    ttype <- "t"
    pval <- 2 * pt(-abs(tstat), df = nobs - rank)
  }
  #
  # aic? TODO
  #
  aic <- NA_real_
  return(list(
    fitted = mu_final,                    # fitted values
    coefficients = setNames(beta, xnms),  # coefficients
    se = se,                              # standard errors
    vcov = vc,                            # variance-covariance matrix
    tstat = tstat,                        # test statistics for coefs
    pval = pval,                          # p-values for coefs
    test_type = ttype,                    # test type for coefs - z or t test
    dispersion = phi,                     # dispersion parameter
    r_response = r_response,              # response residuals
    r_pearson  = r_pearson,               # pearson residuals
    r_deviance = r_deviance,              # deviance residuals
    deviance = dev,                       # model deviance
    deviance_null = dev_null,             # null deviance
    deviance_scaled = dev / phi,          # scaled model deviance
    aic = aic,                            # akaike information criterion
    df_resid = nobs - rank,               # residual degrees-of-freedom
    fam = fam,                            # probability distribution family
    weights = weights,                    # prior weights
    w = w,                                # final working weight matrix diagonal
    it = it                               # number of iterations to convergence
  ))
}

```


```{r}
#| code-fold: true

print_glm <- function(obj) {
  tt <- toupper(obj$test_type)
  test_nm <- paste(tt, "stat")
  p_nm <- paste0("Pr(>|", tt, "|)")
  coefs <- setNames(data.frame(
    obj$coefficients,
    obj$se,
    obj$tstat,
    obj$pval
  ), c("Est.", "SE", test_nm, p_nm))
  rnc <- names(obj$coefficients)
  if (length(unique(rnc)) != length(rnc)) rnc <- NULL
  row.names(coefs) <- if (is.null(rnc)) paste0("X", 1:nrow(coefs)) else rnc

  cat("==== GLM RESULTS ====\n")
  cat("  y|x ~", obj$fam$name_family, "\n")
  cat("  link:", obj$fam$name_link, "\n\n")
  cat("Deviance residuals:\n")
  print(summary(obj$r_deviance))
  cat("\nCoefficients:\n\n")
  print(coefs)
  cat("\nDispersion parameter:", obj$dispersion, "\n")
  cat("---\n\n")
  cat("Null deviance:", obj$deviance_null,
      paste0("(", length(obj$fitted) - 1, " d.o.f.)\n"))
  cat("Residual deviance:", obj$deviance,
      paste0("(", obj$df_resid, " d.o.f.)\n"))
  cat("Number of Fisher Scoring/IRLS Iterations:", obj$it)
}


```


Here is an example of a Bernoulli GLM with the canonical logit link - i.e., logistic regression.

First we need to implement the family-dependent functions for the binomial family.  
In a real world scenario we would want to convert this into an actual class object, or to be more R-like we could implement generic functions, etc. For now we keep it simple. 
(More detail is given later on what exactly is going on in these functions, so feel free to ignore this code for now.)

```{r}
#| code-fold: true

#
# binomial family helpers
#
link.binomial <- function(theta) {
  return(log(theta / (1 - theta)))
}

link_inv.binomial <- function(theta) {
  return(1 / (1 + exp(-theta)))  # sigmoid
}

mu_eta.binomial <- function(eta) {
  ene <- exp(-eta)
  return(ene / (ene + 1)^2)
}

variance.binomial <- function(mu) {
  return(mu * (1 - mu))
}


# inspired by:
# https://github.com/lme4/lme4/blob/master/src/glmFamily.cpp#L64
# implements y * log(y/mu)
.ylogy <- function(y, mu) {
  v <- y / mu
  ix <- v != 0
  v[ix] <- log(v[ix])
  return(y * v)
}
deviances.binomial <- function(y, mu, weights) {
  d <- 2 * weights * (
    .ylogy(y, mu) + .ylogy(1 - y, 1 - mu)
  )
  return(d)
}

# based on binomial()$init
init.binomial <- function(y, weights, fam) {
  n <- fam$params$size
  if (length(n) > 1 && length(n) != length(y)) {
    stop("length(size) must be 1 or equal to length(y)")
  }
  y <- y / n # prop. of successes
  weights <- weights * n # if other weights are also provided, combine
  nw <- if (all(n == 1)) weights else n
  return(list(
    y = y,
    weights = weights,
    mustart = (nw * y + 0.5) / (nw + 1)
  ))
}

# size = number of trials for each observation
# (if bernoulli, size = 1)
init_binomial <- function(size = 1) {
  new <- list(
    name_family = "binomial",
    name_link = "logit",
    link = link.binomial,
    link_inv = link_inv.binomial,
    mu_eta = mu_eta.binomial,
    variance = variance.binomial,
    deviances = deviances.binomial,
    init = init.binomial,
    params = list(
      size = size
    )
  )
  return(new)
}
```

```{r}
# SIM SOME DATA
fam <- init_binomial()
xraw <- cbind(1, rnorm(1e3), rnorm(1e3))
p_true <- 1 / (1 + exp(-1 - 2 * xraw[,2] - 3 * xraw[,3]))
yraw <- rbinom(1e3, size = 1, prob = p_true)
mod_bern <- fit_glm_irls(X = xraw, y = yraw, fam = fam)

cat("Our IRLS coefs:\n", coef(mod_bern))

# stats::glm results
cat("stats::glm coefs:\n",
    coef(glm(yraw ~ -1 + xraw, family = binomial(link = "logit")))
)

```


```{r, echo=FALSE}
rm(fam, xraw, p_true, yraw, mod_bern)
```




## Diagnositcs and Inference

Now we can dive into the steps that follow the IRLS algorithm. These help us evaluate the model's fit and perform inference.  

### Deviance

After estimating the parameters for our model, we will want to evaluate the "goodness of fit", or how well our model represents the data.  

With a classical linear model we may look at the sum of squared residuals as a measure of discrepancy between our data $y$ and our model's fitted values, $\hat{\mu}$ (we often use the notation $\hat{y}$ when discussing fitted values in classical linear regression).  

In the GLM world we consider **deviance**, which generalizes the SSR. The (scaled) deviance takes the form:  
$$
D^*(y, \hat{\mu}) = 2(\ell_{full} - \ell_{model})
$$
$\ell_{full}$ is the log likelihood of the "full" or "saturated" model - i.e., the model with the maximal number of parameters, so a parameter for every observation, making $\hat{\mu} = y$ exactly.  
$\ell_{model}$ is the log likelihood of the model under investigation.  


The formula to calculate the deviance for a family of probability distributions depends on their particular likelihood function, so it is implemented for the family objects.  
As an example, the Gaussian/Normal family's scaled deviance formula is:  
$$
D^*(y, \hat{\mu}) = \sum_i (y_i - \hat{\mu_i})^2 / \sigma^2
$$

Since the scaled deviance $D^*$, when written generally, includes a $\frac{1}{\phi}$ term, we can also use the "un-scaled deviance", $D = \phi D^*$, which essentially ignores the dispersion parameter. Hence, the un-scaled deviance for the Gaussian family is just the sum of squared residuals.

You will notice that R uses the un-scaled deviance in its calculations (see `gaussian()$dev.resids`), so we do the same.

Also, notice that R calls this concept the *residual deviance*, as opposed to the *null deviance*, which compares the saturated model with a null model (which just has an intercept). 

The idea is to compare the deviance of our fitted model with the null deviance, to get a sense of how adding terms improves the model from the intercept-only null model. Further, we can compare the residual deviances across "nested models" - for example, we may want to know if Model 2, with covariates $x_1$, $x_2$, and $x_3$ improves upon Model 1, which just has covariate $x_1$. This is formalized by the *analysis of deviance*, which we won't go into here.  


```{r}
x <- rnorm(100)
y <- rpois(100, lambda = poisson()$linkinv(1 + 2*x))


# full/saturated model
mod_full <- glm(y ~ as.factor(1:length(y)), family = poisson)

# model under investigation
mod_test <- glm(y ~ x, family = poisson)
summ <- summary(mod_test)

# null model
mod_null <- glm(y ~ 1, family = poisson)

# Residual deviance
cat("Residual deviance:\n",
    "'by hand': ", 2 * (logLik(mod_full) - logLik(mod_test)),
    "\n from summary.glm: ",
    summ$deviance)

# Null deviance
cat("Null deviance:\n",
    "'by hand': ", 2 * (logLik(mod_full) - logLik(mod_null)),
    "\n from summary.glm: ",
    summ$null.deviance)

```
```{r, echo=FALSE}
rm(x,y,mod_full,mod_test,mod_null,summ)
```


In our GLM function, the deviance calculations goes:  

```{r, eval=FALSE}
# compute final fitted values using estimated coefficients
mu_final <- c(fam$link_inv(x %*% beta))
#
# residual deviance - based on family specific function
#
dev_vec <- fam$deviances(y, mu_final, weights)
dev <- sum(dev_vec)
#
# null deviance - i.e., that of intercept only model
#
# compute the mean value of y (slightly more complicated if weights are used),
# which is equivalent to the intercept in an intercept only model
wtdmu <- sum(weights * y) / sum(weights)
dev_null <- sum(fam$deviances(y, wtdmu, weights))
```


### Residuals

In classic linear regression we often look at the residuals $y - X\hat{\beta}$ (which in GLM notation, for the Gaussian with identity link, is equivalent to $y - \hat{\mu}$). These are called "response residuals" in the context of GLMs.  

We also have the **Pearson residual**, which is the response residual scaled by the square root of the variance:  
$$
r_P = \frac{y_i - \hat{\mu}_i}{\sqrt{V(\mu_i)}}
$$

However, we typically talk about **deviance residuals** when using GLMs, which take the form:  
$$
r_D = sign(y_i - \hat{\mu_i}) \sqrt{d_i}
$$

- Here, $d_i$ is the deviance of observation $i$. In general the deviance (scaled or un-scaled) can be written $D = \sum_i d_i$ - i.e., it can be broken up into the sum of individual deviances.

- So in the deviance residual, the *direction* of the residual is the same as the response residual, but the *magnitude* is the square root of the deviance for observation $i$.

It's always nice to consider the familiar Gaussian case. Recall from above that for the Gaussian family, $D^* = \sum_i (y_i - \hat{\mu}_i)^2 / \sigma^2$, or the sum of squared residuals over the variance. Therefore, $d_i = (y_i - \hat{\mu_i})^2/\sigma^2$, and $\sqrt{d_i} = (y_i - \hat{\mu_i}) / \sigma$, so that $r_D$ is equivalent to the Pearson residual.  

In our GLM function, this is how residuals are computed:

```{r, eval=FALSE}
# response residual
r_response <- c(y - mu_final)

# pearson residual
r_pearson <- c(r_response / sqrt(fam$variance(mu_final)))

# deviance residual
r_deviance <- c(sign(r_response) * sqrt(dev_vec))
```



### Dispersion

For the binomial and Poisson families, the dispersion parameter $\phi$ is assumed to be 1 (as a result of their particular likelihood functions).

In classic GLMs where $\phi$ is not known, $\phi$ is considered a nuisance parameter because it is not the key parameter of interest (that would be $\mu$) but it is needed for inference.

For example, in classic linear regression $\phi = \sigma^2$ where $Y|X \sim N_n(\mu, \sigma^2 I)$. The sampling distribution of the parameters is $\hat{\beta} \sim N_p(\beta, \sigma^2 (X^\top X)^{-1})$ so $\sigma^2$ is needed for calculation of standard errors, among other statistics.  

Just as we back-calculate $\sigma^2$ as $\hat{\sigma}^2 = \sum_i (Y_i - x_i^\top \hat{\beta})^2 / (n - p)$, we need a general way to back-calculate $\phi$.  

Recall that $Var(Y) = a(\phi) b''(\theta) = \frac{\phi}{w} V(\mu)$ so $\phi = \frac{w Var(Y)}{V(\mu)}$.  

If $\beta$ was know, an unbiased estimate would be:  
$\phi = \frac{1}{n} \sum_{i=1}^n \frac{w_i (y_i - \mu_i)^2}{V(\mu_i)}$

But since $\beta$ must be estimated we obtain:  
$$
\hat{\phi} = \frac{1}{n - p} \sum_{i=1}^n \frac{w_i (y_i - \mu_i)^2}{V(\mu_i)}
$$

In our glm function, dispersion gets calculated as so:  
```{r, eval=FALSE}

# recall the pearson residual from above
r_pearson <- c(r_response / sqrt(fam$variance(mu_final)))

# calculate dispersion
if (fam$name_family %in% c("binomial", "poisson")) {
  phi <- 1
} else {
  # 1 / (n - p) * sum(w * (y - mu)^2 / V(mu)) 
  phi <- sum(weights * (r_pearson)^2) / (nobs - rank)
}

```






### Coefficient Standard Errors

The estimates $\hat{\beta}$, like other maximum likelihood estimates, are asymptotically normal with covariance matrix equal to the inverse of the Fisher information matrix:  
$$
\hat{\beta} \sim N_p(\beta, \phi(X^\top W X)^{-1}) \equiv \hat{\beta} \sim N_p(\beta, I^{-1}) 
$$

Thus, the covariance matrix for $\hat{\beta}$ can be computed as $\phi (X^\top W X)^{-1}$, where we can use the final diagonal weight matrix $\hat{W}$ from the IRLS iterations, and $\hat{\phi}$. 

The standard error for each coefficient $\beta_j$ is the square root of the $j$-th diagonal of this covariance matrix.

Here are the calculations used above:

```{r, eval=FALSE}
#
# variance-covariance matrix
#
# use last w from IRLS iterations 
#     phi * inverse(X' W X)
vc <- phi * solve(crossprod(sqrt(w) * x))
#
# standard errors
#
se <- sqrt(diag(vc))
```


### Tests of Coefficient Significance

As with linear regression, we may want to test the null hypothesis that a coefficient $\beta_j$ is significantly different from 0:  
$H_0 : \beta_j = 0$  
$H_1 : \beta_j \ne 0$

If $\phi$ is known, $\hat{\beta} \sim N(\beta, \phi(X^\top W X)^{-1})$ and we can use a z-test with test statistic:   
$z_j = \frac{\hat{\beta}_j}{\sqrt{\phi (X^\top W X)^{-1}_{jj}}} \sim N(0, 1)$

We compare $|z_j|$ the $1 - \alpha/2$ quantile (for test with level $\alpha$) of $N(0,1)$ and reject the null if the test statistic is more extreme than the pre-specified quantile.  

However, we have to estimate $\phi$ for exponential family distributions other than Poisson or Binomial (for which $\phi = 1$), which changes the asymptotic distribution of the test statistic. If we use an estimated $\hat{\phi}$, the test statistic is distributed according to Student's T distribution with $n - p$ degrees of freedom.

Here are the manual calculations used in our function:  

```{r, eval=FALSE}
#
# H0: beta_j == 0
# H1: beta_j != 0
#
tstat <- beta / se
if (fam$name_family %in% c("binomial", "poisson")) {
  ttype <- "z" # since phi is 1
  pval <- 2 * pnorm(-abs(tstat))
} else {
  ttype <- "t"
  pval <- 2 * pt(-abs(tstat), df = nobs - rank) # n - p d.o.f.
}
```



### Confidence Intervals for the Fitted Values

After we fit a model, we will likely want to observe the fitted values or make predictions based on new data.  

As with classic linear regression, we may also want to report frequentist confidence intervals around the estimated mean $\hat{\mu}$ in addition to the fitted or predicted point estimates.  

For confidence intervals, the key is to create the intervals on the link scale (i.e., in terms of $g(\mu)$) before using the inverse link function to transform them back to the response scale ($\mu$).

To calculate the sampling variance of the fitted mean:  

Since $g(\mu) = X \beta$,  

$$Var(g(\hat{\mu}_i)) = Var(X_i \hat{\beta}) = X_i cov(\hat{\beta}) X_i^\top$$
And we have already calculated $cov(\hat{\beta}) = \phi(X^\top W X)^{-1}$.  
To get the standard error we take the square root.  

To calculate $1 - \alpha$ level confidence intervals on the link scale, we use:  
$g(\mu) \pm t_{1 - \alpha/2, n -p} SE(g(\hat{\mu}))$ where the t-multiplier is the $1 - \alpha/2$ quantile of Student's T distribution with $n - p$ (number of observations - rank) degrees of freedom.   


To convert the standard error of the fitted mean from the link to the response scale, R uses a calculation based on a [delta method](https://en.wikipedia.org/wiki/Delta_method) approximation. I won't go into details here because for the purpose of creating confidence intervals, it is typically recommended to follow the above approach, where the upper and lower bounds are calculated on the link scale and transformed to the response scale by the inverse link function.  

Prediction intervals for GLMs are also more complicated, and not covered here.



```{r}
predict_glm <- function(mod, X, type = c("link", "response"), level = .95) {
  # mod:   Model returned from fit_glm_irls
  # X:     Model/design matrix - new or original
  # type:  Fitted values are either returned on the link scale, as g(mu), or
  #        response scale, as mu.
  # level: The (1 - alpha) level of the t-test used in the lower and upper-bound
  #        calculations
  type <- match.arg(type)
  X <- as.matrix(X)
  beta <- as.matrix(mod$coefficients, ncol = 1)
  eta <- X %*% beta
  mu <- mod$fam$link_inv(eta)
  #
  # Fitted SEs - on link scale
  # i.e., predict(fitglm, newdata = data.frame(X), se.fit = TRUE)
  vc <- mod$vcov # vcov of beta
  se_fit <- sqrt(diag(X %*% vc %*% t(X)))
  #
  # level% confidence interval
	tfrac <- qt((1 - level)/2, df = mod$df_resid)
  out <- list(
    fit = c(eta),
    lwr_ci = c(eta - tfrac * se_fit),
    upr_ci = c(eta + tfrac * se_fit)
  )
  if (type == "response") {
    # convert from link to response scale
    out <- lapply(out, fam$link_inv)
    se_fit <- se_fit * abs(fam$mu_eta(eta))
  }
  out <- append(out, list(se_fit = c(se_fit)))
  return(out)
}

fam <- init_binomial()
X <- cbind(1, rnorm(100))
y <- rbinom(100, 1, p = fam$link_inv(X[, 2]))
mod <-  fit_glm_irls(X, y, fam)
fitglm <- glm(y ~ X[, 2], family = binomial())

# comparisons
distribcmp <- \(x1, x2) `rownames<-`(rbind(summary(x1), summary(x2)),
                                     c("ours", "stats"))


# link scale
distribcmp(
  predict_glm(mod, X)$fit,
  predict(fitglm, data.frame(X))
)

# response scale
distribcmp(
  predict_glm(mod, X, type = "response")$fit,
  predict(fitglm, data.frame(X), type = "response")
)

# standard errors - link scale
distribcmp(
  predict_glm(mod, X)$se_fit,
  predict(fitglm, data.frame(X), se.fit = TRUE)$se.fit
)

# standard errors - response scale
distribcmp(
  predict_glm(mod, X, type = "response")$se_fit,
  predict(fitglm, data.frame(X), se.fit = TRUE, type = "response")$se.fit
)

```

```{r, echo=FALSE}
rm(fam, X, y, mod, fitglm)
```


### Akaike information criterion

R also reports the AIC in `summary.glm`, which is a commonly used estimator of model fit.  
$$
AIC = -2 \ell_{model} + 2p
$$

where $\ell_{model}$ is the log-likelihood of the model under investigation and $p$ is the number of parameters in the model.  

A lower AIC value is considered better, relative to a higher one. Thus, AIC rewards a high log likelihood (recall, the likelihood function measures the likelihood of the parameter(s) given the data we have). Simultaneously, it adds a penalty for the number of parameters so as not to reward overfitting.  

Examples:  

```{r}
# tree height on age
fitglm <- glm(height ~ age, family = Gamma, data = Loblolly)
p <- 3 # intercept and one coefficient, plus the dispersion
l_model <- logLik(fitglm)
c(AIC <- -2 * logLik(fitglm) + 2 * p)
(fitglm$aic)


# number of warp breaks in yarn on type of wool
fitglm <- glm(breaks ~ wool, family = poisson, data = warpbreaks)
p <- 2 # intercept and coefficient
l_model <- logLik(fitglm)
c(AIC <- -2 * logLik(fitglm) + 2 * p)
(fitglm$aic)

```


Since the AIC relies on the specific log likelihood of an exponential family-member, it needs to be implemented for each family. (TODO)


## Example: Bernoulli and Binomial GLMs

Given $\theta = h(X \beta)$, where $h$ is identity under the canonical
link:

The canonical logit link function $g(\mu)$ is $\log(\frac{\mu}{n - \mu}) = \theta$.
Its inverse is the logistic function:
$$
\mu(\theta) = g^{-1}(\theta) = \frac{1}{1 + \exp(-\theta)}
$$

And the derivative of the inverse is:
$$
\mu'(\theta) = \frac{\exp(-\theta)}{(\exp(-\theta) +1)^2}
$$

The variance function of $\mu$ is:
$$
Var(\mu) = \mu(1 - \mu)
$$

For the binomial family, $\phi = 1$.


We can also implement a deviance function to calculate the fitted model's deviance and the deviance residuals.  

For the binomial family, given fitted values $\hat{\mu}$, the deviance for a single observation is:   
$$d_i = 2 w_i ~ (y_i \log(y_i / \hat{\mu}_i) + (1 - y_i) \log[(1 - y_i)/(1 - \hat{\mu}_i)])$$
The total deviance $D$ is the sum of the individuals, and the scaled deviance is $D^* = D / \phi$, but $\phi = 1$.




Note that $Y$ is technically the proportion of successes out of $n$ (or here called "size") independent trials.  
In the special Bernoulli case, we have either a 0 - for 0 out of 1 successes - or a 1 - for 1 out of 1 successes - which gives a simpler interpretation.  

Therefore, this implementation requires $Y$ to be a vector encoding the number of successes for each observation, and additionally we must specify the number of trials when initializing the binomial family object. In the `init.binomial` function I convert from number of successes to proportion of successes.  

You'll notice below that R handles this slightly differently - instead of a response vector we must use a $n \times 2$ response matrix where the left column encodes the number of successes and the right column encodes the number of failures.


```{r}
#
# binomial family helpers
#
link.binomial <- function(theta) {
  return(log(theta / (1 - theta)))
}

link_inv.binomial <- function(theta) {
  return(1 / (1 + exp(-theta)))  # sigmoid
}

mu_eta.binomial <- function(eta) {
  ene <- exp(-eta)
  return(ene / (ene + 1)^2)
}

variance.binomial <- function(mu) {
  return(mu * (1 - mu))
}


# inspired by:
# https://github.com/lme4/lme4/blob/master/src/glmFamily.cpp#L64
# implements y * log(y/mu) without logging 0
.ylogy <- function(y, mu) {
  v <- y / mu
  ix <- v != 0
  v[ix] <- log(v[ix])
  return(y * v)
}
deviances.binomial <- function(y, mu, weights) {
  d <- 2 * weights * (
    .ylogy(y, mu) + .ylogy(1 - y, 1 - mu)
  )
  return(d)
}

# based on binomial()$init
init.binomial <- function(y, weights, fam) {
  n <- fam$params$size
  if (length(n) > 1 && length(n) != length(y)) {
    stop("length(size) must be 1 or equal to length(y)")
  }
  y <- y / n # prop. of successes
  weights <- weights * n # if other weights are also provided, combine
  nw <- if (all(n == 1)) weights else n
  return(list(
    y = y,
    weights = weights,
    mustart = (nw * y + 0.5) / (nw + 1)
  ))
}

# size = number of trials for each observation
# (if bernoulli, size = 1)
init_binomial <- function(size = 1) {
  new <- list(
    name_family = "binomial",
    name_link = "logit",
    link = link.binomial,
    link_inv = link_inv.binomial,
    mu_eta = mu_eta.binomial,
    variance = variance.binomial,
    deviances = deviances.binomial,
    init = init.binomial,
    params = list(
      size = size
    )
  )
  return(new)
}
```


### Simulated Example: Bernoulli

```{r}
# SIM DATA
fam <- init_binomial()
xraw <- cbind(1, rnorm(100), rnorm(100))
p_true <- 1 / (1 + exp(-1 - 2 * xraw[,2] - 3 * xraw[,3]))
yraw <- rbinom(100, size = 1, prob = p_true)
mod_bern <- fit_glm_irls(X = xraw, y = yraw, fam = fam)

print_glm(mod_bern)

fitglm <- glm(yraw ~ xraw[, 2:ncol(xraw)], family = binomial(link = "logit"))
summary(fitglm)


distribcmp <- \(x1, x2) `rownames<-`(rbind(summary(x1), summary(x2)),
                                     c("ours", "stats"))
#
# reponse residuals
distribcmp(mod_bern$r_response,
           resid(fitglm, "response"))

#
# pearson residuals
distribcmp(mod_bern$r_pearson,
           resid(fitglm, "pearson"))

#
# deviance residuals
distribcmp(mod_bern$r_deviance,
           resid(fitglm, "deviance"))
## note: R calculates the deviance differently for a no-intercept model, so if
## you want to pass in a design matrix with the intercept accounted for (like
## glm(y ~ -1 + X) for ex), the null deviance calculation will differ.
## see: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/glm.R#L362

```

```{r, echo=FALSE}
rm(fam, xraw, p_true, yraw, mod_bern)
```


### Example with Data: Logistic regression on kyphosis data

**DATA**:

The kyphosis data samples children who have had corrective spinal surgery.

Variables:

-   kyphosis: indicates if kyphosis, a type of deformation, is present after the operation
-   age: age of patient in months
-   number: the number of vertebrae involved
-   start: the number of the first (topmost) vertebra operated on

We will model the presence of kyphosis as a Bernoulli random variable, so we seek to estimate $p$ - the probability of kyphosis - conditional on covariates.  


```{r}
kyphosis <- read.csv("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/rpart/kyphosis.csv")
names(kyphosis) <- tolower(names(kyphosis))
kyphosis <- transform(kyphosis,
  rownames = NULL,
  present = kyphosis == "present" # binary response
)
xvars <- c("age", "number", "start")
head(kyphosis)


# GLM (logistic regression)
# Y|X ~ Bernoulli(p)
# canonical logit link
fam <- init_binomial()
x <- cbind(intercept = 1, kyphosis[, xvars])
y <- kyphosis$present
mod_kyph <- fit_glm_irls(X = x, y = y, fam = fam)
print_glm(mod_kyph)


fitglm <- glm(present ~ age + number + start,
              family = binomial(link = "logit"),
              data = kyphosis)
summary(fitglm)

# compute on "new" data...
testage <- data.frame(intercept = 1,
                      age = 1:600,                    # reasonable values + some
                      number = mean(kyphosis$number), # hold fixed
                      start = mean(kyphosis$start))   # hold fixed

testage <- cbind(testage,
                 predict_glm(mod_kyph, X = testage, type = "response"))

# testage <- within(testage, {
#   bs <- coef(mod_kyph)
#   ses <- mod_kyph$se
#   # ugly but proves a point
#   pred_logit <- bs[1] + age * bs[2] + number * bs[3] + start * bs[4]
#   pred <- fam$link_inv(pred_logit)
#   rm(bs, ses)
# })

ggplot(testage, aes(x = age)) +
  geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), alpha = 0.25) +
  geom_line(aes(y = fit)) +
  geom_point(data = kyphosis,
             aes(x = age, y = as.integer(present), color = present)) +
  coord_cartesian(xlim = c(0, 400)) +
  theme(legend.position = "none") +
  labs(title = "Modeled Probability of Kyphosis",
       x = "Age (months)",
       y = "P")

```


### Simulated Example: Binomial Proportions

```{r, warning=FALSE}
size <- 30
fam <- init_binomial(size = size)
xraw <- cbind(1, rnorm(100), rnorm(100))
p_true <- fam$link_inv(1 + 2 * xraw[,2] + 3 * xraw[,3])
yraw <- rbinom(100, size = size, prob = p_true)

# Binomial Model: Y|X ~ Binomial(30, p)
# include some weights for demonstration...
mod_binom <- fit_glm_irls(X = xraw, y = yraw, fam = fam, weights = 100)
print_glm(mod_binom)

# compared to stats::glm - same results:
#
# left col is no. of successes, right col is no. of failures
summary(glm(cbind(yraw, size - yraw) ~ xraw[, 2:ncol(xraw)], family = binomial(),
            weights = rep(100, 100)))


# alt: provide size in weights - but causes some differences on the back end
summary(glm(yraw / size ~ xraw[, 2:ncol(xraw)], family = binomial(),
            weights = size * rep(100, 100)))

```


### Example with Data: Binomial regression with the *Oribatid mites* data

The data set is found and described in detail here:  
[David Zeleny community ecology data - mites](https://www.davidzeleny.net/anadat-r/doku.php/en:data:mite)



Oribatid mites are a diverse group of small, soil-dwelling arthropods.
This data set is composed of 70 cores of mostly Sphagnum mosses collected in Canada in June 1989. The columns record the abundance of 35 species.  
Additional data sets include environmental and positional variables, which are merged below.

We will look at mite species 1 and model the proportion of species 1 out of all species in each given core as a binomial random variable.  
The size parameter (typically denoted $n$) is the sum of all mites observed in the core, across all species. We seek to estimate $p$, the proportion of species 1, conditional on covariates.  


```{r}
mite <- cbind(
  read.delim('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/mite_xy.txt'),
  read.delim('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/mite_env.txt'),
  read.delim('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/mite.txt')
)

(nms <- names(mite))
xvars <- c("SubsDens", "WatrCont")

mite <- within(mite, {
  total <- rowSums(mite[, grep("Spec", nms)])
  spec1 <- Spec01
  not_spec1 <- total - Spec01
})
mite <- mite[, c("total", "spec1", "not_spec1", xvars)]

head(mite)


# MODEL
# Y|X ~ Binomial(n = total, p = ?)

X <- cbind(1, mite[, xvars])
y <- mite[, "spec1"]
fam <- init_binomial(size = mite$total)
mod_mite <- fit_glm_irls(X, y, fam = fam)

print_glm(mod_mite)


# COMPARE WITH STATS::GLM
fitglm <- glm(cbind(spec1, not_spec1) ~ SubsDens + WatrCont,
              family = binomial(),
              data = mite)
summary(fitglm)


# PLOT
Xnew = data.frame(
  intercept = 1,
  SubsDens = mean(mite$SubsDens),
  WatrCont = seq(min(mite$WatrCont)-10, max(mite$WatrCont)+10, length = 100)
)
mite_fit <- cbind(Xnew,
                  predict_glm(mod_mite, Xnew, type = "response"))

ggplot(mite_fit, aes(x = WatrCont, y = fit)) +
  geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), alpha = 0.45) +
  geom_line() +
  geom_point(data = mite, aes(x = WatrCont, y = spec1/total)) +
  labs(title = "Modeled Proportion of Mite Species 1",
       subtitle = "Fit and 95% Confidence Interval",
       x = bquote("Water Content of Moss Core (g"~ dm^-3~")"),
       y = "Proportion") +
  theme_bw()


```





## Example: Poisson IRLS

Given $\theta = h(X \beta)$, where $h$ is identity under the canonical
link:

The canonical log link function $g(\mu)$ is $\log(\mu) = \theta$.  

Its inverse is:
$$
\mu(\theta) = g^{-1}(\theta) = \exp(\theta)
$$

And the derivative of the inverse is:  
$$
\mu'(\theta) = \exp(\theta)
$$

And the variance as a function of $\mu$ is just $\mu$:

$$
V(\mu) = \exp(\theta) = \mu
$$

That is, the mean and variance are equal for the Poisson distribution.

The deviance of a single observation for the Poisson family takes form:  
$$
d_i = 2 w_i (y_i \log(y_i / \hat{\mu}_i) - (y_i - \hat{\mu}_i))
$$

The (un-scaled) residual deviance is, again, $D = \sum_i d_i$.  

For the poisson family, $\phi = 1$.



```{r}
#
# poisson family helpers
#
link.poisson <- function(theta) {
  theta[theta == 0] <- .Machine$double.eps
  return(log(theta))
}

link_inv.poisson <- function(theta) {
  return(pmax(exp(theta), .Machine$double.eps))
}

mu_eta.poisson <- function(theta) {
  return(pmax(exp(theta), .Machine$double.eps))
}

variance.poisson <- function(mu) {
  return(mu)
}

.ylogy <- function(y, mu) {
  v <- y / mu
  ix <- v != 0
  v[ix] <- log(v[ix])
  return(y * v)
}

deviances.poisson <- function(y, mu, weights) {
  d <- 2 * weights * (
    .ylogy(y, mu) - y + mu
  )
  return(d)
}

# based on possion()$init
init.poisson <- function(y, weights, fam) {
  return(list(
    mustart = y + 0.1,
    weights = weights,
    y = y
  ))
}

init_poisson <- function() {
  new <- list(
    name_family = "poisson",
    name_link = "log",
    link = link.poisson,
    link_inv = link_inv.poisson,
    mu_eta = mu_eta.poisson,
    variance = variance.poisson,
    deviances = deviances.poisson,
    init = init.poisson,
    params = list()
  )
  return(new)
}
```


Here is a simulated example:  


```{r}
# SIM DATA
fam <- init_poisson()
xraw <- cbind(1, rnorm(1e3), rnorm(1e3))
lambda_true <- exp(1 + 2 * xraw[,2] + 3 * xraw[,3])
yraw <- rpois(1e3, lambda = lambda_true)
mod_pois <- fit_glm_irls(X = xraw, y = yraw, fam = fam)
print_glm(mod_pois)

summary(glm(yraw ~ xraw[, 2:ncol(xraw)], family = poisson(link = "log")))

```


### Example with Data: Poisson regression with the *Oribatid mites* data

Sticking with the mites data for simplicity, we can model the number of occurrences of species 1 as a Poisson random variable, meaning we want to estimate the parameter $\lambda$, which is the average count of species 1, conditional on covariates. 


```{r}
# MODEL
# Y|X ~ Poisson(lamda)
head(mite)

X <- cbind(1, mite[, xvars])
y <- mite[, "spec1"]
fam <- init_poisson()
mod_mite2 <- fit_glm_irls(X, y, fam = fam)

print_glm(mod_mite2)


fitglm <- glm(spec1 ~ SubsDens + WatrCont, family = poisson(), data = mite)
summary(fitglm)

# PLOT
Xnew = data.frame(
  intercept = 1,
  SubsDens = mean(mite$SubsDens),
  WatrCont = seq(min(mite$WatrCont)-10, max(mite$WatrCont)+10, length = 100)
)
mite_fit <- cbind(Xnew,
                  predict_glm(mod_mite2, Xnew, type = "response"))

ggplot(mite_fit, aes(x = WatrCont, y = fit)) +
  geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), alpha = 0.45) +
  geom_line() +
  geom_point(data = mite, aes(x = WatrCont, y = spec1)) +
  labs(title = "Modeled Count of Mite Species 1",
       subtitle = "Fit and 95% Confidence Interval",
       x = bquote("Water Content of Moss Core (g"~ dm^-3~")"),
       y = "Count") +
  theme_bw()


```


# TODO:

- Implement AIC?

- More examples in a new document (Poisson, Gaussian, ...)

- Offsets example with Poisson,  
https://stats.stackexchange.com/questions/175349/in-a-poisson-model-what-is-the-difference-between-using-time-as-a-covariate-or



