---
title: "Linear Regression and QR Decomposition"
author: "Hans Elliott"
format:
  html:
    embed-resources: true
toc: true
---

Notes based on McCullagh and Nelder (M&N), 1983, section 3.8.2 and [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/orthogonal-sets.html) (ILA).  

Notes on Householder reflections based on *Generative Additive Models*, Wood (2017), among other sources.  


# Intro & Motivation

Recall that least squares problems have a closed form solution:  
$\hat{\beta} = (X^\top X)^{-1} X^\top Y$  
as long as $X^\top X$ is invertible.  

In practice, stats software does not typically compute coefficients using this formula. Instead, a variety of algorithms exist which can make the computation more efficient and numerically stable.  


First, consider that the *condition number* of a matrix is a measure of closeness to singularity. Recall, a singular matrix is non-invertible. Large values of the condition number can lead to numeric instability from rounding errors. Since $X^\top X$ has a condition number that is the square of the condition number of $X$, it is useful to avoid using/having to invert $X^\top X$.  

Numeric instability is demonstrated by this example from 
https://genomicsclass.github.io/book/pages/qr_and_regression.html:  


```{r}
n <- 50; M <- 500
x <- seq(1, M, len=n)
X <- cbind(1,x,x^2,x^3)
colnames(X) <- c("Intercept","x","x2","x3")
beta <- matrix(1, nrow = ncol(X), ncol = 1)
set.seed(1)
y <- X %*% beta + rnorm(n,sd=1)

# solve for beta...
try(solve(crossprod(X)) %*% crossprod(X,y))
```


Because of the way that $X$ is constructed, there is a mix of relatively small values and very large values. When we compute $X^\top X$, some of the values become huge. When we try to invert $X^\top X$ (with `solve`), we are essentially going to end up computing determinants, and some will become 0 due to floating point limitations (when we divide by a very big determinant).

We can see that the reciprocal condition number is very close to 0, which is a sign that inverting the matrix won't go well.  

```{r}
head(X)
crossprod(X)

# reciprocal condition number (1 / condition number)
rcond(X)
rcond(crossprod(X))
```



# QR Decomposition

A popular method for performing least squares, and that used by R, uses a technique called QR factorization or decomposition.  


The aim is to decompose $X \in n \times p$ ($n \le p$) into the product of 2 matrices: the $n \times p$ orthogonal matrix $Q$ and the $p \times p$ upper right triangle matrix $R$, where $R_1$ will denote the upper right triangle of $R$.  

We can motivate the procedure statistically:  
If $y$ is the outcome vector with mean $X \beta$ and variance $\sigma^2 I$, we can make an orthogonal transformation to new variables $u$ defined by:  
$u = Q^\top y$.  

Then, the new mean and variance are:  

- $E[U] = Q^\top E[Y] = Q^\top X\beta = Q^\top Q R \beta = R\beta = R_1 \beta$  
  - Since the last $n - p$ components of $U$ have 0 expectation (due to $R$ being an upper right triangle matrix), they give no information about $\beta$.  
  - Thus, we just need to estimate the first $p$ components of $u$.
- $cov[U] = \sigma^2 Q^\top I Q = \sigma^2 I$  

Thus, 

$$
R_1 \hat{\beta} = u \\
\Rightarrow R_1 \hat{\beta} = Q^\top y \\
\Rightarrow \hat{\beta} = R_1^{-1} Q^\top y
$$

This is easily solved because there is a simple inversion algorithm for triangular matrices (more on that later).  



So far we can see that we need $Q$, to compute $u = Q^\top y$, and we need $R$ to compute $R_1 \hat{\beta} = u$.  

- Technically, it is not necessary to compute $Q$ explicitly because if $y$ is appended to $X$ (so the matrix has form $\{X : y\}$) the sequence of operations that takes $X$ to $R$ also transforms $\{X : y\}$ to $\{R : u\}$.  
This means the first $p$ rows of the augmented matrix $\{R : u\}$ can be used to get the coefficients. (M&N, pg. 87). 

There are a few methods of finding $Q$ and $R$, but a common method is the Gram-Schmidt method, which orthogonalizes the columns of the design matrix $X$.  

## Gram-Schmidt

The Gram-Schmidt procedure is presented in linear algebra as a method for obtaining an orthogonal basis for a subspace, which makes computations easier, particularly when they involve projections (recall that linear regression involves the orthogonal projection of $Y$ onto the column space of $X$). 

In the linear regression scenario, the subspace of interest is the column space of $X$ and by finding its orthogonal basis we can construct the $n \times p$ orthogonal matrix $Q$ (and as a byproduct, we will compute $R$).

First, recall some key points from linear algebra (ILA):  

  - In linear algebra, a basis of a vector space is a set of linearly independent vectors that spans the entire space.  
  - An orthogonal basis is such a set where the basis vectors are mutually orthogonal.  
    - A set of vectors $\set{u_1, ..., u_n}$ is called **orthogonal** if $u_i \cdot u_j = 0$ whenever $i \ne j$ (picture each vector being perpendicular to one another, like the x, y, z axes in $\mathbb{R}^3$).  
    - Also note that an **orthonormal** basis is an orthogonal basis of unit vectors.  

  -  An orthogonal basis of $W$ has a simple formula for orthogonally projecting a vector $x$ onto it:  
    $x_W = \frac{x \cdot u_1}{u_1 \cdot u_1} u_1 + ... + \frac{x \cdot u_m}{u_m \cdot u_m} u_m$.   
      - Note that if the basis is orthonormal, the denominators are always 1.  

  - A subspace $W \in \mathbb{R}^n$ has an orthogonal complement $W^\perp$, which is the set of all vectors $v$ in $\mathbb{R}^n$ that are orthogonal to all the vectors in $W$.
  - **Orthogonal decomposition:** for any vector $x$ in $\mathbb{R}^n$, if $W$ is a subspace of $\mathbb{R}^n$ then we can write $x = x_W + x_{W^\perp}$, where $x_W$ is the closest vector to $x$ on $W$ (i.e., the orthogonal projection of $x$ onto $W$) and $x_{W^\perp}$ is a vector in $W^\perp$ (the orthogonal complement to $W$).  


The **Gram-Schmidt** process is defined as so:    

Let $\set{v_1, ..., v_m}$ be a basis for a subspace $W$ of $\mathbb{R}^n$. 
To construct the orthogonal basis $\set{u_1, ..., u_m}$ for $W$,  
define:  

1. $u_1 = v_1$  
2. $u_2 = (v_2)_{\text{Span}(u_1)^\perp} = v_2 - \frac{v_2 \cdot u_1}{u_1 \cdot u_1} u_1$   
3. $u_3 = (v_3)_{\text{Span}(u_1, u_2)^\perp} = v_3 - \frac{v_3 \cdot u_1}{u_1 \cdot u_1} u_1 - \frac{v_3 \cdot u_2}{u_2 \cdot u_2} u_2$  

$\cdots$  

m. $u_m = (v_m)_{\text{Span}(u_1, \dots, u_{m-1})^\perp} = v_m - \sum_{i=1}^{m-1} \frac{v_m \cdot u_i}{u_i \cdot u_i} u_i$

  - First note that $u_1 = v_1$ is clearly in $\text{Span}(v_1)$. $u_2$ is a linear combination of $v_2$ and $u_1$, so it is in $\text{Span}(v_1, v_2)$. Like this, we can show that each $u_i$ is in $\text{Span}(v_1, \dots, v_i)$ and thus each $u_i$ is in $W$ (important considering the $u_i$ will be an (orthogonal) basis for $W$).  
  - Now note that $\set{u_1, \dots, u_{i - 1}}$ is an orthogonal set, by definition, since each vector $u_i$ after $u_1$ is defined as the projection of $v_i$ onto orthogonal complement of the span of all of the previous $u_j$.
    - Recall, the orthogonal complement of the span $\text{Span}(u_1, \dots, u_{m-1})$ is the set of all vectors in $\mathbb{R}^n$ that are orthogonal to all of the vectors in the span. So, projection of a vector from a subspace into its orthogonal complement implies the vector is orthogonal to the original subspace.  
    - Thus, as we build up the set of $u_i$, each addition is orthogonal to all of the previous additions.  
  - The orthogonal set is an orthogonal basis for the subspace spanned by $\text{Span}(u_1, \dots, u_{i - 1})$, which itself is a subspace of $W$.  
  - For simplicity, call $W^* = \text{Span}(u_1, \dots, u_{i - 1})$. This is the subspace spanned by all of the previous columns, expressed using the orthogonal basis instead of the original basis.  
  - From the orthogonal decomposition theorem, we can write $v_i = v_{i_{W^*}} + v_{i_{W^{*^\perp}}}$.  
    - $v_{i_{W^*}}$ is the orthogonal projection of $v_i$ onto subspace $W^*$. Because we have the orthogonal basis for $W^*$ from the previous iterations, this is simple to compute. For the $m$-th basis vector,    
    $v_{m_{W^*}} = \sum_{i=1}^{m-1} \frac{v_m \cdot u_i}{u_i \cdot u_i} u_i$  
    - $v_{i_{W^{*^\perp}}}$ is the vector in ${W^*}^\perp$, the orthogonal complement of $W^*$ (think of it as the distance vector between the vector $v_i$ and  $v_{i_{W^*}}$, i.e., the residual).
  
  - Thus, to get the next orthogonal vector, we rewrite the orthogonal decomposition formula:  
  $u_m = v_{m_{W^{*^\perp}}} = v_m - v_{m_{W^*}} = v_m - \sum_{i=1}^{m-1} \frac{v_m \cdot u_i}{u_i \cdot u_i} u_i$
    - Note that if the $\set{u_1, \dots, u_{m-1}}$ are *orthonormal*, this simplifies further because $u_i \cdot u_i = 1$ (so the denominator can be ignored).  


## Obtaining the orthogonal matrix $Q$

Finally, note that we we can also use Gram-Schmidt to compute an *orthonormal basis*, which is an orthogonal basis of unit vectors (the magnitude of each vector is 1).  

The difference is just that after computing each $u_i$, we convert them to unit vectors $e_i$ by dividing by the Euclidean norm:  
$e_i = \frac{u_i}{||u_i||}$,  
where $||u_i|| = \sqrt{\sum_{j=1}^n (u_i)_j^2 }$  

So in QR decomposition for linear regression, our subspace of interest $W$ is the column space of $X$, and the original basis vectors $\set{ v_1, \dots, v_m}$ are just the columns of $X$.  


After performing the Gram-Schmidt process, the resulting set of orthonormal basis vectors $\set{e_1, \dots, e_m}$ are the columns of $Q$.    


## Obtaining the upper right triangular $R$

Once we have computed $Q$, it is straight forward to compute $R$ because $Q^\top X = Q^\top Q R = R$.  

- which is allowed because $Q$ being orthonormal implies that $Q^\top Q = I$, and in particular that $Q$ has a left inverse $Q^\top$.  
- [notes](https://uregina.ca/~franklam/Math415/Math415_QR_Factorization.pdf)   

We can think of each entry in $R$ as undoing the operations that took $X$ to $Q$ (since $X = QR$).  

We can also compute $R$ during the Gram Schmidt iterations if we consider what each component of the upper right triangular will be:  

- Given the orthonormal basis vectors $\set{e_1, \dots, e_m}$, and original basis vectors (i.e., the columns) $\set{v_1, \dots, v_m}$ the upper right triangular matrix has rows:  

$$
\begin{bmatrix}
v_1 \cdot e_1 & v_2 \cdot e_1 & \dots & v_m \cdot e_1 \\
0 & v_2 \cdot e_2 & \dots & v_m \cdot e_2 \\
\dots & \dots & \dots & \dots \\
0 & 0 & \dots & v_m \cdot e_m
\end{bmatrix}
$$


Note that for the diagonals, $v_i \cdot e_i = ||u_i||$

- This is easy to show for the first orthogonal basis vector/first row of $R$:  
$v_1 \cdot e_1 = u_1 \cdot (\frac{u_1}{||u_1||}) = \frac{u_1 \cdot u_1}{||u_1||} = \frac{||u_1||^2}{||u_1||} = ||u_1||$

- This is convenient because we also needed to divide each column vector in $Q$ by $||u_i||$ to convert them to an orthonormal basis.  


# QR Decomposition in R

```{r}
qr_decomp <- function(X, y = NULL) {
  stopifnot(is.matrix(X))
  if (!is.null(y)) stopifnot(is.matrix(y),
                             nrow(y) == nrow(X),
                             ncol(y) == 1)
  # Gram-Schmidt
  n <- nrow(X)
  p <- ncol(X)
  Q <- matrix(0, nrow = n, ncol = p)
  R <- matrix(0, nrow = p, ncol = p)
  for (j in 1:p) {
    v <- X[, j, drop = FALSE] # v_j
    u <- v # u_i, step 1
    if (j > 1) {
      for(i in 1:(j-1)) {
          #          (u_i . v_j)
          R[i, j] <- t(Q[, i, drop = FALSE]) %*% v
          #    v - Sum_i proj_u_i(v_j) * (u_i), iteratively
          u <- u - R[i, j] * Q[ ,i]
      }
    }
    #         ||u_j||
    R[j, j] = sqrt(crossprod(u))
    #   e_j = u_j / ||u_j||
    Q[ , j] = u / R[j, j]
  }
  # create u and solve for beta
  #   u = Q'y; R b = u
  #   b = R.inv u
  if (!is.null(y)) {
    u <- crossprod(Q, y)
    beta <- backsolve(R, u)
  } else {
    beta <- NA_real_
  }
  return(list(Q = Q, R = R, beta = drop(beta)))
}
```


```{r, echo=FALSE, eval = FALSE}
#TMP
x <- c(1,2,3,1,3,9,1,9,10,1,4,5)
X <- matrix(x, 4, 3, byrow = TRUE)
y <- matrix(1:4, nrow = 4)
out <- qr_decomp(X,y)
format(out$beta, scientific = F)


(mod <- lm(y ~ -1 + X))
(sigma(mod))


# SE calculation
# since vcov = sigma^2 * (t(X) %*% X)^(-1)
(xtx <- diag(vcov(mod) / sigma(mod)^2))

R <- out$R
(var <- diag(solve(crossprod(out$R))))


Rinv <- backsolve(R, diag(ncol(R)))
diag(tcrossprod(Rinv))


# manual backsolve
m <- ncol(R)
Rinv <- matrix(0, nrow(R), ncol(R))
y <- diag(ncol(R))
for (j in seq.int(m, 1)) {
  Rinv[j, ] <- y[j, ]
  if (j < m) {
    for (i in seq.int(j+1, m)) {
      cat("j:", j, ", i:", i, "\n")
      cat("row:", Rinv[j, ], "\n")
      Rinv[j, ] <- Rinv[j, ] - R[j, i] * Rinv[i, ]
      # beta[j] <- beta[j] - R[j, i] * beta[i]
    }
  }
  Rinv[j, ] <- Rinv[j, ] / R[j, j]
  # beta[j] <- beta[j] / R[j, j]
}

Rinv
diag(tcrossprod(Rinv))

```

Simulated example:  

```{r}
x <- cbind(1, rnorm(100), rnorm(100))
y <- x[,1] + 2 * x[,2] + 3 * x[, 3] + rnorm(100, sd = 0.25)

qr <- qr_decomp(x, as.matrix(y))

# compare to R
qr$beta
coef(lm(y ~ -1 + x))
```


We could also compute $\beta = R^{-1} u$ using `solve` to invert $R$, but that would be less numerically stable.  
Works fine in this case:  

```{r}
u <- t(qr$Q) %*% as.matrix(y)
c(beta <- solve(qr$R) %*% u)
```

As noted above, if we apply gram schmidt to the augmented matrix that includes $y$, we don't need to explicitly use $Q$ to solve for $\beta$:  

```{r}
(Ru <- qr_decomp(X = cbind(x, y))$R)
# u = Q %*% y, which is the last colum, without the last row
(u <- Ru[-nrow(Ru), ncol(Ru)])
# The rest of the matrix is the right triangular R
(R <- Ru[-nrow(Ru), -ncol(Ru)])
c(beta <- backsolve(R, u))
```


Note the orthonormal vectors of $Q$:  
```{r}
x <- cbind(rnorm(2), rnorm(2))
(Q <- qr_decomp(x)$Q)

# magnitude: expect 1
c(
  norm(Q[, 1, drop = FALSE], type = "2"),
  norm(Q[, 2, drop = FALSE], type = "2")
)

# product of orthogonal vectors should be 0
Q[, 1] %*% Q[, 2]


plot(t(Q), #transpose to use columns as coordinates
     xlim = c(-1, 1), ylim = c(-1,1),
     xlab = "obs1", ylab = "obs2")
arrows(0,0, x1 = Q[1,1], y1 = Q[2,1])
arrows(0,0, x1 = Q[1,2], y1 = Q[2,2])

```


# Inverting an upper right triangular matrix

As explained nicely by [wikipedia](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution), we can compute the inverse of any triangular matrix via forward/back substitution (in this case, back substitution, which is what `backsolve` accomplishes in the `qr_decomp` function above).  

For our matrix equation of form $R \beta = u$, where $R \in p \times p$ is an upper right triangular, we have a system of linear equations:  
$R_{1,1}\beta_1 + R_{1,2}\beta_2 + \dots + R_{1,p} \beta_p = u_1$  
$0 + R_{2,2}\beta_1 + \dots + R_{2, p} \beta_p = u_2$   

$\cdots$  

$0 + \dots + 0 + R_{p-1, p-1} \beta_{p-1} + R_{p-1, p} \beta_p = u_{p-1}$  
$0 + \dots + 0 + 0 + R_{p,p} \beta_p = u_p$

Thus, we can solve the last equation for $\beta_p = u_p / R_{p,p}$ and then plug this result into the previous equation:  
$\beta_{p-1} = u_{p-1} - \frac{R_{p-1, p} \beta_p}{R_{p-1, p-1}}$
and continue on until we have solved for all of the components of $\beta$.  

So going backward from the last equation (row) to the first, at each step $j$ we do:    

- At step $j = p$, we compute:  
$\beta_j = u_j / R_{j,j}$.  
- At every other step $j < p$ (going backwards) we compute:  
$\beta_j = u_j - \sum_{i = j + 1}^{p} \frac{R_{j,i} \beta_{i}}{R_{j,j}}$
  - For example, say $R \in 3 \times 3$.  
  $\beta_3 = u_3 / R_{3,3}$  
  $\beta_2 = u_2 - \sum_{i = 3}^{3} \frac{R_{2,3} \beta_3}{R_{3,3}} = u_2 - \frac{R_{2,3}\beta_3}{R_{3,3}}$  
  $\beta_1 = u_1 - \sum_{i = 2}^3 \frac{R_{1,i} \beta_i}{R_{1,1}}$  



```{r}
x <- cbind(1, rnorm(100), rnorm(100))
y <- as.matrix(x[,1] + 2 * x[,2] + 3 * x[, 3] + rnorm(100, sd = 0.25))
qr <- qr_decomp(x, y)

(R <- qr$R)
(u <- t(qr$Q) %*% y)

#
# backsolve for beta
p <- ncol(R)
beta <- numeric(p)
for (j in seq.int(p, 1)) {
  beta[j] <- u[j]
  if (j < p) {
    for (i in seq.int(j+1, p)) {
      beta[j] <- beta[j] - R[j, i] * beta[i]
    }
  }
  beta[j] <- beta[j] / R[j, j]
}

cat("Manual backsolve results\n",
    beta,
    "\nR's backsolve results\n",
    qr$beta, "\n")

```



# Standard Errors

To compute the covariance matrix, $\sigma^2 (X^\top X)^{-1}$, we can use:  

$(X^\top X)^{-1} = ((QR)^\top QR)^{-1} = (R^\top Q^\top Q R)^{-1} = (R^\top R)^{-1}$.  

```{r}
X <- cbind(1, rnorm(100), rnorm(100))
Y <- x[,1] + 2 * x[,2] + 3 * x[, 3] + rnorm(100, sd = 0.25)

qr <- qr_decomp(X, as.matrix(Y))
Q <- qr$Q
R <- qr$R

rtr.inv <- solve(crossprod(R))
diag(rtr.inv)

# compare with R
# (divide by sigma^2 to get back to (X'X)^-1)
mod <- lm(Y ~ -1 + X)
diag(vcov(mod) / (sigma(mod)^2))

```
However, the above example uses `solve` to do matrix inversion, and we can do better by computing the inverse in a more creative way.  

The **Choleski decomposition** is another matrix decomposition method which aims to find the lower-triangular $p \times p$ matrix $L$ that satisfies:  
$X^\top X = LL^\top$,  
making $L$ a square root matrix of $X^\top X$.  

Then, the inversion of $(X^\top X)$ is accomplished via formula:  
$(X^\top X)^{-1} = (L^{-1})^\top L^{-1}$

Since $L$ is a triangular matrix, there is a simple algorithm (forward substitution) to invert it, which makes $(L^{-1})^\top L^{-1}$ simple to calculate.  


Now,note that:  
$R^\top R = R^\top Q^\top Q R = X^\top X$,  
so that $R$ is the upper triangular Choleski square-root matrix of $X^\top X$.  
In particular, $R$ is the transpose of $L$ from the Choleski decomposition.  
So $(L^{-1})^\top L^{-1} = ((R^\top)^{-1})^\top (R^\top)^{-1} = ((R^{-1})^\top)^\top (R^{-1})^\top = R^{-1} (R^{-1})^\top$ (using that $(A^\top)^{-1} = (A^{-1})^\top$).  

Thus, to compute $(X^\top X)^{-1}$ for the standard errors we can do:  
$$
(X^\top X)^{-1} = R^{-1} (R^{-1})^\top
$$


```{r}
Rinv.r <- backsolve(R, diag(ncol(R)))
diag(tcrossprod(Rinv.r))

#
# or manually compute R inverse via back substitution
m <- ncol(R)
Rinv <- matrix(0, nrow(R), ncol(R))
y <- diag(ncol(R))
for (j in seq.int(m, 1)) {
  Rinv[j, ] <- y[j, ]
  if (j < m) {
    for (i in seq.int(j+1, m)) {
      Rinv[j, ] <- Rinv[j, ] - R[j, i] * Rinv[i, ]
    }
  }
  Rinv[j, ] <- Rinv[j, ] / R[j, j]
}

Rinv
diag(tcrossprod(Rinv))



# # note, using the lower tri and forward solving also works, but an extra transpose
# L <- t(R)
# Linv <- forwardsolve(L, diag(ncol(L)))
# diag(crossprod(Linv))
```

Note that this is essentially what R's `chol2inv` does for us:  

```{r}
diag(chol2inv(R))
```

# Fitted Values

It is clear that given $X = Q R$ and $\hat{\beta} = R^{-1} Q^\top y$:  
$$
\hat{y} = X \beta = Q R R^{-1} Q^\top y = Q Q^\top y  
$$

```{r}
head(drop(tcrossprod(Q) %*% Y))
head(fitted(mod))
```

Of course, we are usually solving for $\beta$, and once we have that we could just compute $\hat{y} = X \beta$.  



# Alternative Approach: Householder Reflections

The Gram-Schmidt method is numerically stable, but an alternative approach to QR decomposition that utilizes Householder reflections is more efficient (because we do not need to actually form the $Q$ matrix) and gives us some other advantages (e.g., combined with pivoting we have a test for rank deficiency).  



## Householder matrices

Notes from General Additive Models, Simon Wood (2nd Edition), section B.5 - B.8.  

Orthogonal matrices are square matrices which rotate or reflect vectors.  

An important example are the Householder ("reflector") matrices.  

For a non-zero vector $u$, a Householder matrix is 
$$
H = I - \gamma u u^\top
$$
where $\gamma = 2 / ||u||^2$

- $H$ is symmetric.  
- Multiplication of a vector by $H$ can be completed without explicitly storing $H$. Instead, $u$ and $\gamma$ are stored and then $H x$ is evaluated by computing:  
$\alpha \leftarrow u^\top x$  
$x \leftarrow x - \alpha \gamma u$  
  - This only requires $O(n)$ operations - $n = dim(x)$ - where formation and use of $H$ is $O(n^2)$.    
- An interesting property of $H$ is that if $x$ and $y$ are 2 non-zero vectors of same length ($||x|| = ||y||$) then the Householder matrix such that $Hx = y$ is obtained by setting $u = x - y$.  



## QR Decomposition using Householder Matrices

Householder reflections and QR decomposition via Householder reflections are well explained by this short lecture series on [YouTube](https://youtube.com/playlist?list=PLxKgD50sMRvBHxvNPnGQ1kEHlO5y7mSnh&si=O3l1rEm2qttr4YFe).  

Again, any $n \times p$ matrix $X$ ($n \le p$) and be written as $X = QR$ where $Q$ is an orthogonal matrix and $R$ is a $p \times p$ upper triangular.  

$Q$ can be made up of the product of $p$ Householder matrices through the following steps:  

- First construct the Householder matrix $H_1$ which reflects/rotates the first column of $x$ so that all of its elements except the first are zeroed.  
- Then calculate $H_2$ which will transform the second column of $X$ so that its first element is unchanged, and every element beyond the second is zeroed.  
  - Since $H_2$ only acts on elements from the second row down, the first row is unchanged and the first column is unchanged - since elements in column 1 below the first row are already zeroed.  
- Continue until we reach the situation where: $$H_p H_{p-1} \cdots H_1 X = R$$  
- Hence, since $X = QR$, then $Q^\top = H_p H_{p-1} \cdots H_1$ implying that $Q = H_1 H_2 \cdots H_p$.  
  - Thus, $Q$ can be stored as a series of $p$ householder matrix components.


Recall from above that $Hx$ can be evaluated by computing $\alpha = u^\top x$ and then computing $x = x - \alpha \gamma u$, where $\gamma = 2 / ||u||^2 = 2 / u^\top u$.  
Thus, we must determine how to construct $u$.  


From [wikipedia](https://en.wikipedia.org/wiki/QR_decomposition#Using_Householder_reflections):  
The goal of Householder reflections for QR decomposition is to find a linear transform that changes the vector $x$ into a vector of the same length which is collinear with the unit vector $e_1 = [1, 0, \dots, 0 ]^\top.$:  

<img src="./householder_wiki.png" width="20%">

Thus, we want $Hx = \alpha e_1$ for some constant $\alpha$.  

Since $H$ is orthogonal, $||Hx|| = ||x||$ and $||\alpha e_1 || = |\alpha|||e_1|| = |\alpha|$, which implies $\alpha = \pm ||x||$.  
The sign is selected so that is has the opposite sign of the $k$-th coordinate of $x$, where $x_k$ is to be the pivot coordinate (e.g., for the first Householder matrix $H_1$, $x_k = x_1$.)  

- (This is to avoid [loss of significance](https://en.wikipedia.org/wiki/Catastrophic_cancellation), a phenomenon where "subtracting good approximations to two nearby numbers may yield a very bad approximation to the difference of the original numbers...inherent to subtraction, when the inputs are approximations themselves", such as in floating-point arithmetic).  

Thus, if $x$ is the $k$-th column of $X$, we compute the $k$-th coordinate of $u$ as $u_k = x_k + sign(x_k) ||x|| e_k$ (or, to make the use of opposite sign clear, $u_k = x_k - (-sign(x_k)) ||x|| e_k$).    
Components before and after the $k$-th remain unchanged (this is evident since $e_k$ is a vector that is 0 everywhere except for the $k$-th element, which is 1).  

- In practice, we can avoid the need for multiplication by the $e_k$ vector by subsetting $X$ in each iteration so that $x'$ represents the rows $k$ through $n$ of the $k$-th column of $X$. Then, we compute $u_1 = x'_1 + sign(x'_1)||x'||$ 



This [example](https://rpubs.com/aaronsc32/qr-decomposition-householder) is helpful in illustrating the process.  
Let us calculate the decomposition of:  
$$
A =
  \begin{bmatrix}
  2 & -2 & 18 \\
  2 & 1 & 0 \\
  1 & 2 & 0  \\
  \end{bmatrix}
$$
First we need to find a reflection that transforms the first column $x = a_1$ into $||x|| e_1 = |\alpha| e_1 = [\alpha, 0,0]^\top$.  

Now $u = x + \alpha e_1$, and we know $|\alpha| = ||x||$, which in this case is $\sqrt{2^2 + 2^2 + 1^2} = 3$. (We can say $\alpha = -3$ since $x_1=3 > 0$, or simply add the $sign(x_1)\alpha e_1$ vector instead of substract).  
So $u = [2, 2, 1]^\top + 3 [1,0,0]^\top = [5,2,1]^\top$.  

Since $H_1 = I - 2 \frac{u u^\top}{u^\top u}$ we can plug in $u$ to solve:  
$$
H_1 =
  \begin{bmatrix}
  -2/3 & -2/3 & -1/3 \\
  -2/3 & 0.733 & -0.133 \\
  -1/3 & -0.133 & 0.933
  \end{bmatrix}
$$

Note that:  

$$
H_1 A =
  \begin{bmatrix}
  -3 & 0 & -12 \\
  0 & 1.8 & -12 \\
  0 & 2.4 &  -6  \\
  \end{bmatrix}
$$
So applying the first Householder reflection matrix to the input $A$ gets us close to our upper triangular matrix.  

- Note that since $H_1 = (I - 2 \gamma u u^\top)$, then $H_1 A = (I - 2 \gamma u u^\top) A = (A - (\gamma u)^\top(A u))$, which is how we will iteratively update $A$ in the code below to produce the upper trianguar $R$, without having to explicitly compute any Householder matrices.   

In R:
```{r}
A <- matrix(c(2,2,1, -2,1,2, 18,0,0), 3, 3)
n <- nrow(A)
p <- ncol(A)

k <- 1
# extract kth sub-column
x_k <- A[k:n, k]
#
# calculate the norm
nrm <- drop(sqrt(crossprod(x_k)))
#
# calculate reflection vector u
u <- x_k
u[1] <- u[1] + sign(u[1]) * nrm
u
#
# calculate gamma = 2 / ||u||^2
gamma <- 2 / drop(crossprod(u))
#
# calculate H for example 
H1 <- diag(n - k + 1) - gamma * tcrossprod(u)
H1
#
# update sub-matrix of A via householder reflection
A[k:n, k:p] <- A[k:n, k:p] - tcrossprod(gamma * u, crossprod(A[k:n, k:p], u))
A
```

Next, we can take the $(1,1)$ minor (shown below) and repeat this process.    
$$
A' =
  \begin{bmatrix}
  1.8 & -12 \\
  2.4 & -6 \\
  \end{bmatrix}
$$
The Householder vector is $u = [4.8, 2.4]^\top$ and
the Householder matrix is the below matrix, which we pad with a diagonal to maintain dimensions:  
$$
H_2 = 
  \begin{bmatrix}
  1 & 0 & 0 \\
  0 & -0.6 & -0.8 \\
  0 & -0.8 & 0.6  \\
  \end{bmatrix}
$$


```{r}
k <- 2
# extract kth sub-column
x_k <- A[k:n, k]
#
# calculate the norm
nrm <- drop(sqrt(crossprod(x_k)))
#
# calculate reflection vector u
u <- x_k
u[1] <- u[1] + sign(u[1]) * nrm
u
#
# calculate gamma = 2 / ||u||^2
gamma <- 2 / drop(crossprod(u))
#
# calculate H' for example 
H2 <- diag(nrow = n, ncol = p)
H2[k:n, k:n] <- diag(n - k + 1) - gamma * tcrossprod(u)
H2
#
# update sub-matrix of A via householder reflection
A[k:n, k:p] <- A[k:n, k:p] - tcrossprod(gamma * u, crossprod(A[k:n, k:p], u))
A
```

Finally, we move to the last sub-matrix which is just $[6]$ at this point:  

```{r}
k <- 3
# extract kth sub-column
x_k <- A[k:n, k]
#
# calculate the norm
nrm <- drop(sqrt(crossprod(x_k)))
#
# calculate reflection vector u
u <- x_k
u[1] <- u[1] + sign(u[1]) * nrm
u
#
# calculate gamma = 2 / ||u||^2
gamma <- 2 / drop(crossprod(u))
#
# calculate H' for example 
H3 <- diag(nrow = n, ncol = p)
H3[k:n, k:n] <- diag(n - k + 1) - gamma * tcrossprod(u)
H3
#
# update sub-matrix of A via householder reflection
A[k:n, k:p] <- A[k:n, k:p] - tcrossprod(gamma * u, crossprod(A[k:n, k:p], u))
A
```


Thus we can compute the orthogonal matrix $Q$ from QR decomposition as :  
$$
Q = H_1^\top H_2^\top H_3^\top = 
  \begin{bmatrix}
  -2/3 & 2/3 & -1/3 \\
  -2/3 & -1/3 & 2/3 \\
  -1/3 & -2/3 & -2/3
  \end{bmatrix}
$$

```{r}
Q <- crossprod(H1, crossprod(H2, H3))
Q
```

And we computed $R$ iteratively by transforming $A$ as:
$$
R = H_3 H_2 H_1 A
$$
```{r}
R <- A[1:p, ]
R
```


## Algorithm in R: QR via Householder Reflections

Here we will use Householder reflections to iteratively transform the input matrix into $R$. We will also store $u$ and $\gamma$, the components of each Householder matrix $H_k$, to be used for other computations to demonstrate that we never need to explicitly compute $Q$.  

```{r}
# sim some data
x <- cbind(1, rnorm(100), rnorm(100))
y <- x[,1] + 2 * x[,2] + 3 * x[, 3] + rnorm(100, sd = 0.25)
mod_test <- lm(y ~ -1 + x)


#
# QR DECOMP BY HOUSEHOLDER REFLECTIONS
#
A <- x
n <- nrow(A)
p <- ncol(A)

us <- matrix(0, n, p)
gammas <- numeric(p)
for (k in 1:p) {
  #
  # compute components of Householder matrix H_k
  #
  x_k   <- A[k:n, k]                  # k-th sub-column
  nrm   <- drop(sqrt(crossprod(x_k))) # euclidean norm
  u     <- x_k                        # reflection vector
  u[1]  <- u[1] + sign(u[1]) * nrm
  gamma <- 2 / drop(crossprod(u))     # gamma = 2 / ||u||^2
  #
  # update sub-matrix of A via householder reflection
  #
  A[k:n, k:p] <- A[k:n, k:p] - tcrossprod(gamma * u, crossprod(A[k:n,k:p], u))
  #
  # store components of H_k
  #
  us[k:n, k] <- u
  gammas[k] <- gamma
}
#
# finalize upper triangular R
R <- A[1:p, ]
R[lower.tri(R)] <- 0 # elements that are essentially 0 are zeroed
R

#
# can compute Q (if you wanted to explicitly obtain Q)
#  via Householder reflections
Q <- diag(n)
for (k in 1:p) {
  Q[,k:n] <- Q[,k:n] - tcrossprod(Q[,k:n,drop=FALSE] %*% us[k:n, k], gammas[k] * us[k:n, k])
}
Q <- Q[, 1:p]


all.equal(R, qr.R(qr(x)))
all.equal(Q, qr.Q(qr(x)))
```


Now that we have obtained $R$ and have the components to perform calculations that require $Q$, we can see how to use these components to finish our least squares estimation.  

We want to compute $Q^\top y$ so we can solve $R \beta = Q^\top y$ for $\beta$.  

We saw that $Q^\top = H_p H_{p-1} \cdots H_1$, and we know that we can compute any $H_k x$ using just $u$ and $\gamma$ by setting $\alpha = u_k^\top x$ and then updating $x = x - \alpha  \gamma_k u_k$:  

```{r}
#
# next we want to compute Q^\top y so we can solve R beta = Q^\top y
z_test <- drop(crossprod(Q, y))
              
#
# But for Householder matrices, can compute Q^\top y without using Q:
# since Q = H_1 ... H_p, and Q^\top = H_p ... H_1
# and we can compute H_k y using just u and gamma
z <- y
for (k in 1:p) {
  alpha <- drop(crossprod(us[k:n, k], z[k:n]))
  z[k:n] <- z[k:n] - alpha * gammas[k] * us[k:n, k]
}
(z <- z[1:p])

all.equal(z_test, z)
```

Now that we have computed $R$ and $Q^\top y$, we can solve for $\hat{\beta} = R^{-1} (Q^\top y)$, which as we saw above is simple thanks to the back-substitution inversion algorithm for an upper right triangular matrix (manual example above).   

```{r}
(beta <- backsolve(R, z))

all.equal(beta, unname(coef(mod_test)))
```

For standard errors, we do not need $Q$ since we saw that $(X^\top X)^{-1} = (R^\top R)^{-1} = (R^{-1})(R^{-1})^\top$ (the last formulation is useful because, again, it is convenient to invert an upper-right triangular matrix).  

```{r}
Rinv <- backsolve(R, diag(nrow(R))) # invert upper right
(XtXinv <- tcrossprod(Rinv))

all.equal(XtXinv,
          unname(vcov(mod_test) / sigma(mod_test)^2))
```

To compute fitted values we could compute $Q Q^\top y$ as we saw above, but then we have to construct $Q$. At this point, it is faster to simply compute $\hat{y} = X \hat{\beta}$.  

```{r}
all.equal(x %*% beta,
          as.matrix(fitted(mod_test)),
          check.attributes = FALSE)
```




Note: more efficient way to store $u$ vectors since they get smaller with each iteration?  

- https://math.stackexchange.com/questions/231557/calculating-number-of-non-zero-elements-in-a-lower-triangular-matrix  
-  https://stackoverflow.com/questions/27380024/how-to-efficiently-store-a-triangular-matrix-in-memory  


## Pivoting

Wood (section B.8, pg. 423) explains that *pivoting* - or reordering the columns of the input matrix - can improve the stability of computations involving QR.  

Pivoting is performed iteratively as the QR decomposition progresses.  

- The column with the largest Euclidean norm is first swapped into the first column.  
- After the first Householder reflection has been applied, the norms of the remaining columns - below the first row - are computed and the column with the largest norm is swapped into the second column.  
- The next Householder reflection is applied, and then the norms of the remaining columns below the second row are computed, and so on...  


A consequence of this approach is that any rank-deficiency is manifested as a trailing zero block of the triangular factor ($R$), giving a way to test for rank deficiency.  



```{r}
qr_householder <- function(A, y = NULL,
                           pivot = TRUE,
                           # (same default as all.equal.numeric)
                           tol = sqrt(.Machine$double.eps)
                           ) {
  #
  # QR DECOMP BY HOUSEHOLDER REFLECTIONS
  #
  n <- nrow(A)
  p <- ncol(A)

  us <- matrix(0, n, p) # store u_k vector for H_k 
  gammas <- numeric(p)  # store gamma_k scalar for H_k
  col_order <- 1:p
  for (k in 1:p) {
    #
    # compute column norms and pivot
    #
    if (pivot) {
      norms <- apply(A[k:n, col_order][, k:p, drop = FALSE],
                     MARGIN = 2,
                     norm, type = "2") # euclidean norm
      piv <- which.max(norms)
      ## swap column order
      tmp <- col_order[k]
      col_order[k] <- col_order[k:p][piv]
      col_order[k:p][piv] <- tmp
      nrm <- norms[piv]
    }
    #
    # compute components of Householder matrix H_k
    #
    x_k   <- A[k:n, col_order][, k]     # k-th sub-column with largest norm
    nrm   <- if (pivot) norms[piv] else drop(sqrt(crossprod(x))) # euclid norm
    u     <- x_k                        # reflection vector
    u[1]  <- u[1] + sign(u[1]) * nrm
    gamma <- 2 / drop(crossprod(u))     # gamma = 2 / ||u||^2
    #
    # update sub-matrix of A via householder reflection
    #
    Asub <- A[k:n, col_order][, k:p]
    A[k:n, col_order][, k:p] <- Asub - tcrossprod(gamma * u, crossprod(Asub, u))
    #
    # store components of H_k
    #
    us[k:n, k] <- u
    gammas[k] <- gamma
  }
  #
  # finalize upper triangular R
  R <- A[1:p, col_order]
  R[lower.tri(R)] <- 0 # lower tri elements that are essentially 0 are zeroed
  #
  # check for linear dependence
  sing <- NULL
  if (pivot) {
    rr <- which(abs(diag(R)) < tol)
    (rank <- p - length(rr))
    if (rank < p) {
      warning("Singular fit detected.")
      sing <- TRUE
    } else {
      sing <- FALSE
    }
    if (rank == 0)
      stop("Input matrix is of rank 0.")
  }
  beta <- NULL
  if (!is.null(y)) {
    #
    # compute Q'y
    z <- y
    for (k in 1:rank) {
      alpha <- drop(crossprod(us[k:n, k], z[k:n]))
      z[k:n] <- z[k:n] - alpha * gammas[k] * us[k:n, k]
    }
    z <- z[1:rank]
    #
    # backsolve for beta = (R)^-1 Q' y
    beta <- backsolve(R[1:rank, 1:rank], z)
    # TODO vectorize
    coefs <- rep_len(NA, p)
    for (i in seq(p)) coefs[col_order[i]] <- beta[i]
  }
  return(list(
    R = R,
    us = us,
    gammas = gammas,
    coefficients = coefs,
    rank = rank,
    pivot = col_order,
    pivoted = pivot
  ))
}

```


First lets test the function on a full rank matrix:  

```{r}
x <- cbind(1, rnorm(100), rnorm(100))
y <- x[,1] + 2 * x[,2] + 3 * x[, 3] + rnorm(100, sd = 0.25)
mod_test <- lm(y ~ -1 + x)

hqr <- qr_householder(x, y)

all.equal(hqr$R,
          qr.R(qr(x, LAPACK = TRUE))) # LAPACK routine does pivot

# note - only LINPACK (ie, when LAPACK = FALSE), checks for linear dependencies
all.equal(hqr$rank,
          qr(x)$rank)

all.equal(hqr$coefficients,
          unname(coef(mod_test)))

setNames(hqr$coefficients,
         paste0("X", seq(ncol(x))))
```


Now test the function on a rank deficient matrix, where the third covariate is a linear combination of the second covariate.  

```{r}
# sim some data - with rank deficiency
x <- cbind(1, rnorm(100))
x <- cbind(x, x[, 2] * 2) # x3 = 2 * x2
y <- x[,1] +  x[,2] + x[, 3] + rnorm(100, sd = 0.25)

# error expected:
# calculating a regression in R will error if we don't allow for singular fits 
try(
  lm(y ~ -1 + x, singular.ok = FALSE)
)

# error expected:
# trying to solve by brute force, we see it fails since the matrix is not invertible!
try(
  crossprod(solve(crossprod(x)), crossprod(x, y)) # b = (X'X)^-1 X'y
)

# by default lm will not error but will essentially drop the third covariate
mod_test <- lm(y ~ -1 + x)
coef(mod_test)
```


In the function, we test for a singular fit by determining if any of the diagonals are essentially 0.

```{r}
hqr <- qr_householder(x, y)

# last diagonal element is essentially 0
hqr$R
```


Note that the coefficients differ slightly from the output of `lm`. Specifically, while the intercept is the same, the estimated slope is different - R's function estimated a slope for X2 and our function estimated a slope for X3. We produced an estimate for X3 instead of X2 due to the results of our particular pivoting implementation.  
It isn't of huge importance, since a singular fit implies that there are an infinite number of equally good solutions to this linear system.  

```{r}
coef(mod_test)

setNames(hqr$coefficients,
         paste0("X", seq(ncol(x))))
```



(Notice that the coefficient R estimates for X2 is exactly twice as big as the coefficient we estimate for X3, and in our simulation X3 = 2 * X2, so we would expect its coefficient to be exactly half the size.)  
```{r}
##
unname(coef(mod_test)[2] / hqr$coefficients[3])
```


If we repeat the regression but remove one of the collinear covariates, we can get back to a unique solution.  
```{r}
qr_householder(x[, -2], y)$coeff

qr_householder(x[, -3], y)$coeff
```


