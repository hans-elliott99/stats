---
title: "Principal Components Analysis"
format:
  html:
    embed-resources: true
toc: true
---

```{r}
library(ggplot2)

```


# Algorithm

Recall:  
Empirical covariance matrix:  

$$
S = \frac{1}{n} \Sigma_{i=1}^n X_i X_i^\top - \bar{X}\bar{X}^\top \\\
~~~~~~~~~~~~~= \frac{1}{n} \Sigma_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^\top \\\
$$  

Or, if we let $X$ be the $n \times d$ matrix of covariates, and $\it{1}$ be the $n \times 1$ vector of all 1s, then:  

$$
S = \frac{1}{n} X^\top X - \frac{1}{n^2} X^\top \it{1}\it{1}^\top X  
$$



```{r}
# we have some data, and we want to reduce dimensions
head(state.x77)
X <- as.matrix(state.x77)
X <- scale(X, center = TRUE, scale = TRUE)

# compute covariance matrix
# v slight differences from `cov(X)` - Bessel's correction? Or just float point?
covmat <- function(M) {
  M <- as.matrix(M)
  n <- nrow(M)
  ones <- matrix(1, nrow = n, ncol = 1)
  S <- (1/n) * (t(M) %*% M) - (1/(n^2)) * (t(M) %*% ones %*% t(ones) %*% M)
  return(S)
}

S <- covmat(X)

# eigen/spectral decomposition
## singular value decomposition - `svd(X)` - would also work
s <- eigen(S)
eigenvals <- s$values
eigenvecs <- s$vectors


# sort eigenvectors by eigenvalues, decreasing
# already done for us!
order(eigenvals, decreasing = TRUE)

# project X onto the span of each eigenvector
Y <- X %*% eigenvecs


# # (to do it like the lecture notes)
# k <- 2 ## n dims to keep
# Y <- matrix(nrow = nrow(X), ncol = k)
# Pk <- matrix(eigenvecs[, 1:k], nrow = nrow(eigenvecs))
# for (i in 1:nrow(X)) {
#   Xi <- matrix(X[i, ], nrow = ncol(X))
#   Y[i, ] <- t(Pk) %*% Xi
# }


dat <- data.frame(Y)
dat$id <- rownames(X)
ggplot(dat, aes(x = X1, y = X2)) +
  geom_text(aes(label = id), alpha = 0.7) +
  theme_minimal()
```


```{r}
# compared to native R solution
# same but different signs, has no practical impact
# (prcomp uses `svd` internally, slight differences in procedure from `eigen`)
dat2 <- data.frame(prcomp(state.x77, scale. = TRUE)$x)
dat2$id <- rownames(dat2)
ggplot(dat2, aes(x = -PC1, y = PC2)) +
  geom_text(aes(label = id), alpha = 0.7) +
  theme_minimal()

```

# Proportion of Explained Variance

```{r}
total_variance <- sum(eigenvals)
tve <- data.frame(
  component  = seq(1, length(eigenvals)),
  eigenvalue = eigenvals,
  proportion = eigenvals / total_variance,
  cumulative = cumsum(eigenvals / total_variance)
)
print(tve)

# scree plot
ggplot(tve, aes(x = component, y = proportion)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(1, nrow(tve))) +
  labs(title = "Variance Explained by Each Component",
       x = "component",
       y = "proportion of total") +
  theme_bw()

```



# Interpreting the Components

In the lecture, there is an example of a PCA analysis where someone ran PCA and then tried to interpret each component as a category. Such an analysis results in a qualitative/descriptive output that might inform a business strategy (or maybe policy for this particular dataset?). 


It makes some sense to do this - imagine you only have 2 features so there are only 2 elements in each component vector (i.e., each eigen vector of the covariance matrix is 2 x 1). Then you could plot them each as a point in the xy-plane, where the x-axis corresponds to feature 1 and the y-axis corresponds to feature 2.  
If the first component vector has a relatively small first coordinate and relatively large second coordinate, than the resulting point will be close to the x-axis and away from the y-axis, implying the second feature is more important for this coordinate. The opposite is true if the first coordinate is large relative to the second coordinate.  
Further, if we consider that the components become the coefficients that multiply with the values of $X$ to produce $Y$ ($Y = Xp$), it seems intuitive to try to investigate them as we might a linear regression.  

This implies a strategy of looking at the elements in each component vector and trying to determine how the values which are largest in magnitude work together to suggest some natural group.  

An alternative approach appears to be to calculate the correlation between the $Y_i$'s and the $X_i$'s (i.e., `cor(X, Y)`), and to interpret the correlations.  
See: <https://online.stat.psu.edu/stat505/lesson/11/11.4>  

There is apparently a whole sub-field (factor analysis) doing things like this. The idea is to make the principal components more interpretable by rotating them so that, while there correlations are preserved, a "simpler structure" is found where different factors tend to load different variables. Larger and smaller values are exaggerated so differences become more apparent.  

See:  

- [What is the intuitive reason behind doing rotations in Factor Analysis/PCA & how to select appropriate rotation?](https://stats.stackexchange.com/questions/151653/what-is-the-intuitive-reason-behind-doing-rotations-in-factor-analysis-pca-how)  

- [PCA slides](https://www3.cs.stonybrook.edu/~mueller/teaching/cse564/Lec%2017%20-%20Principal%20Component%20Analysis.pdf)  

- [How to compute varimax-rotated principal components in R?](https://stats.stackexchange.com/questions/59213/how-to-compute-varimax-rotated-principal-components-in-r)  

- [Principal components and factor analysis](https://pjbartlein.github.io/GeogDataAnalysis/lec16.html#rotation-of-principal-components)



For simplicity, I will just look at the raw components.
Below are the first 3 components. We could try to interpret them, for example: the second component has large values for Population, Income, and Area.  
Maybe we will call this the population-income-area (?) component, and we can say that state's that have a large second component ($(XP)_{i,2}$) can be grouped together and categorized as such. Or maybe we just want to say that population-income-area is an important way to categorize states.  

We might be able to perform a better analysis if we rotate the components.


```{r}
# inspecting the components
rownames(eigenvecs) <- colnames(X)
eigenvecs[, 1:3]

```










